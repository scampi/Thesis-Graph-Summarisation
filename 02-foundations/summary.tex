\chapter{Graph Summarisation}
\label{chap:summary}

In many Computer Science areas, the information is represented as and is analysed through graphs. Graphs are used to capture the social relationship between people, to represent processes and their transition in concurrency, or to structure data in a flexible way, e.g., using the Object Exchange Model \cite{papakonstantinou:1995:oea} or, more recently, the Resource Description Framework \cite{rdfconcepts}.

A common challenge encountered when analysing a graph is its volume. A large number of nodes or edges reduces the scalability and increases the running time of algorithms applied on the data. A solution is to ``translate'' the graph into another smaller graph while preserving its structure. That process is called \emph{graph summarisation} and the resulting graph is then considered as a \emph{summary} of the entity graph.

To the volume is added the challenge of the graph structure being (partially) unknown. Even though an ontology describing the structure might be available, it is not followed strictly. This prevents upstream operations such as querying or query optimisation to be performed efficiently. A graph summarisation may then be used for shedding light on the structure of the graph.

\section{Data Profiling}

A system for the discovery of datasets is proposed in \cite{khatchadourian:2010:eswc}.

\section{Graph Exploration}

\# Approximate Homogeneous Graph Summarisation

An Information Theoretic approach to graph summarisation is presented in \cite{zheng:ipsj:2011}. The authors propose three criteria for defining the homogeneous property of a graph. A criterion can then be relaxed in order to produce a smaller (approximate) graph summary. The technique seeks to lower the entropy of the graph summary, hence ensuring a good quality.

\# Schemex:

The authors in \cite{konrath:jws:2012} propose a three-layer index on top of the Linking Open Data (LOD). Each layer provides a view on the data with varying details. Ultimately, the index allows to select data sources that are relevant for a SPARQL query. The selection is driven by the three layers of the index, where each is more suited to a query of specific complexity.
Each layer of the proposed index provides views of the LOD graph with varying granularity: the lower the layer, the more detailed it is. Each layer can be considered as a possible summarisation of the LOD graph, although connections between sumnodes within a layer is not provided. The proposed index model is then orthogonal to our graph schema model, each layer being a graph schema itself.

\section{Data Analytics}

\# OLAP on Graph:

OLAP (Online Analytical Processing) is a technique for answering analytical queries over multi-dimensional data. Through different operations it allows a user to analyse the data from different perspectives. Each perspective is called a \emph{cube}, obtained by grouping the values of some dimensions. Several works investigate the use of OLAP over graphs: data records analysed through OLAP are now related to each other. The main difference between traditional OLAP and graph OLAP is the \emph{measure} which is traditionally a number becomes a \emph{graph}.

Chen et. al propose in \cite{chen:icdm:2008} two OLAP frameworks \emph{I-OLAP} and \emph{T-OLAP} for analysing graphs. However, the aggregation studied I-OLAP is aimed at several graphs representing the same entities; therefore, it is a merge of the graphs that keeps the entity layer. In a follow-up work \cite{qu:dasfaa:2011}, T-OLAP is further investigated. Along with \cite{zhao:sigmod:2011}, these two works introduces a graph OLAP framework which aggregates nodes into sumnodes, thus changing the topology of the graph. Compared to graph summarisation, these works are not intended to retain the structure of the graph, e.g., by considering incoming edges.

\#\# RDF Analytics:

Colazzo et al. \cite{colazzo:www:2014} describe a framework for using OLAP over RDF data, leveraging the rich structure of such graphs. Each cube is built based on SPARQL queries which define sumnodes and sumedges. This analytical framework is flexible enough to accommodate many kinds of analysis, since the aggregate function is also based on a SPARQL query. Although the analysis of the data is done through its cubes by a user, the computation of a cube is always done over the original data. The need of writing SPARQL queries suggests that a user possesses some knowledge about the structure of the graph. This work is orthogonal to the one proposed in this thesis: our approach highlights the graph structure, thanks to which a deeper analysis of the graph can be done.

\section{Query Optimisation}


The survey \cite{you:2013:towards} analyses several graph summarisation approaches and proposes a classification for these approaches. Three categories are defined according to what aspect of the graph is used for the summarisation:
\begin{enumerate}
\item \emph{attribute-based} approaches consider the label of nodes for the summarisation;
\item \emph{structure-based} approaches rely on the relationships between nodes to compute a summary; and
\item \emph{hybrid} approaches consider both labels and relationships between nodes.
\end{enumerate}

\section{Summarisation for Graph Schema Rendering}

Graph clustering techniques aim to group together nodes of the graph that are ``similar'' with each other, where nodes in a clusters are highly connected, while the connection between clusters is sparse~\cite{Schaeffer:2007:SGC}. Typical applications of graph clustering include the detection of communities in social networks or the analysis of protein interactions.

\cite{Milner:1989:CC:534666} introduces the concept of bisimulation to define equivalent processes in concurrent systems. The relational coarsest partition \cite{Paige:1987:TPR:37185.37186} is generally the algorithm used for computing a bisimulation on a graph. With a similar perspective in reducing the size of the graph, DataGuides \cite{goldman1997dataguides} is the first work to improve query execution through the use of an index on the summary of the structure of the graph.

However, the DataGuides construction algorithm and the bisimulation are computationally expensive. Also, in presence of data with a complex structure, the size of the summary can be as large as the data graph, or even larger with DataGuides. This issue is discussed in \cite{goldman1999approximate}, where some constraints of DataGuides are relaxed, e.g., the existence of a path in the data graph.

The complexity of both real-world data graphs and summarisation algorithms highlight the need for approximate summaries, i.e., summaries ideally smaller and simpler to compute, at the price of errors with regards to the original data graph. \cite{Milo:1999:ISP:645503.656266} extends the work on DataGuides, using the notion of bisimulation to build a structure index. The authors propose to reduce further the size of a summary by defining the type of query to answer. However, it assumes some knowledge about the structure of the data graph in order to specify the query type. Also, any change in the query specification requires to rebuild the summary. In \cite{kaushik:de:2002,kaushik:2002:cib} the definition of bisimulation on a graph is simplified by limiting the length of a path to $k$ hops. In \cite{chen:2003:dia}, the authors vary the maximum length of a path per node, depending on the query load of the system. The authors of \cite{navlakha:2008:gsb} propose a summarisation based on the Information Theory principle of Minimum Description Length. Although the original data graph can be re-created from the summary, a user-defined bounded error parameter can be used to balance the summary volume with the loss in precision. The authors of \cite{Tran:2012:kde} uses a similar approach to \cite{kaushik:2002:cib} for the purpose of improving the partitioning of RDF data and the execution of queries. \cite{tian:sigmod:2008} proposes an approach based on bisimulation that provides more or less detailed summaries. However, the level of detail is left to the user; this assumes some  prior knowledge about the data structure and content. In this paper, we focus instead on approaches that require no prior knowledge.

Current approaches of the summary computation seek to reduce the complexity of algorithms or the volume of the summary. However, the existing approaches and applications \cite{DBLP:conf/esws/KhatchadourianC10,jarrar:2012,Christodoulou:2013:SIL:2457317.2457328}, do not scale to large data graphs composed of billions of nodes and edges. \cite{kbisim-map} proposes an implementation of bisimulation over MapReduce. The iteration aspect of the bisimulation necessitate to read and write the data graph across the network of computing machines. However, this creates an important IO load on the network, therefore increasing the runtime of the algorithm. In this paper, we are interested instead in summarisation algorithms optimised for shared-nothing environment, in which the computation complexity scale gracefully regardless of the structure heterogeneity of the data graph.

\# Distributed Graph Summarisation

In order to deal with large graphs, distributed implementations of graph summarisation has been researched.
Liu et al. in \cite{liu:cikm:2014} present a graph summarisation approach based the \emph{Message Passing} paradigm: the algorithm iteratively merges nodes of the graph until a predefined number of nodes are left. Different solutions for selecting which nodes to merge are discussed, and a method based on \emph{Locality Sensitive Hashing} is introduced. The merge process introduces errors with regards to the entity graph, i.e., either missing or additional edges.

\section{Graph Summarisation Quality}

The index creation in \cite{konrath:jws:2012} is a stream-based approach. The authors investigate then the impact of the window size on the quality of the created index. However, the authors do not investigate the quality with regards to the structure of the data graph summarised by the index.
