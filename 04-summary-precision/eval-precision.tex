\section{Evaluation}
\label{sec:eval}

In this section, we evaluate the trade-off between the performance of an implementation of a summarisation relation, the compression ratio, and the precision of the graph summary.
In this evaluation, we consider the following:
\begin{itemize}
	\item we want to summarise a graph $G=\left\langle V, A, l_V \right\rangle$;
	\item the result of summarising the graph $G$ is the summary graph $\mathcal{S} = \left\langle \mathcal{W}, \mathcal{B}, l_{\mathcal{W}} \right\rangle$ according to the summarisation relation $R$;
	\item with regards to the classification in Section~\ref{sec:error-classification}, we measure the error in the created summary by comparing it to the bisimulation summary $\mathcal{S}_{fbt}$ of $G$, i.e., $\mathcal{S}_{fbt} = \left\langle \mathcal{W}_{fbt}, \mathcal{B}_{fbt}, l_{\mathcal{W}_{fbt}} \right\rangle$ according to the summarisation relation $R_{fbt}$; and
	\item The bisimulation summarisation relation described in Section~\ref{chap:summary:bisim} maps nodes of the graph to a same sumnode if they all share the same outgoing and incoming paths. The bisimulation summarisation relation is smaller with regards to the partial order $\sqsubseteq$ than any of the approximate graph summarisation presented in Section~\ref{sec:approximate} since its summarisation relation is more restrictive. Therefore, we assume the existence of a relation $S \subseteq \mathcal{W}_{fbt} \times \mathcal{W}$ that maps the bisimulation summary to a summary generated from the summarisation relation $R$, i.e., such that $R_{fbt} \sqsubseteq R$.
\end{itemize}

\subsection{Design}
\label{sec:eval:design}

We describe in this section the design of the evaluation. We first present the environment of our experimental framework. Then, we describe the dimensions of our evaluation.

\subsubsection{Environment}

The bisimulation summary $\mathcal{S}_{fbt}$ is the most precise summary of the entity graph, since all incoming and outgoing (and combination of) paths in the summary do exist in the entity graph $G$. In this evaluation, we use the forward bisimulation $R_{fb}$ summarisation relation presented in Section~\ref{chap:summary:bisim} as the \emph{gold standard} summary of an entity graph. This gold standard summary ensures that all outgoing paths do exist. The presented summarisation relations have been implemented using the Hadoop\footnote{Hadoop: \url{http://hadoop.apache.org/}} MapReduce framework.
% Our Hadoop cluster is composed of 10 machines.

\subsubsection{Evaluation Dimensions}

We present in this section three dimensions we use for evaluating a summary, i.e., the summary \emph{compression}, the computational \emph{performance} of the summarisation relation, and the \emph{precision} of the summary.

\begin{quotation}
	\item[\emph{Summary compression.}]

	A summary is smaller if its size $\vert \mathcal{B} \vert$ (number of sumedges) and its order $\vert \mathcal{W} \vert$ (number of sumnodes) are inferior to the size $\vert A \vert$ and the order $\vert V \vert$ of the graph it was created from.

	Apart from the \emph{unique type} summary $R_{ut}$, the presented summarisation relations in Section~\ref{sec:approximate} create a summary which is always smaller or equal to the graph, since there is a one-to-one mapping from the entity graph to the summary.

	The \emph{unique type} summarisation relation $R_{ut}$ may assign several sumnode to a same node $u \in V$ when the set of types is greater than one, i.e., when $\vert types(u) \vert > 1$. The order of a \emph{unique type} summary $\mathcal{S}_{ut} = \left\langle \mathcal{W}_{ut}, \mathcal{B}_{ut}, l_{\mathcal{W}_{ut}} \right\rangle$ is at most equal to the number of types in the graph, i.e., $\vert \mathcal{W}_{ut} \vert = \vert \{ l_{\mathcal{W}_{ut}}(c) \mid \forall (u, c) \in V^2_{ut}\; (u, \glssymbol{atype}, c) \in \mathcal{B}_{ut} \wedge \glssymbol{atype} in T \} \vert$. However, in the worst case, its size can equal $\vert \mathcal{B}_{ut} \vert = \vert \mathcal{W}_{ut} \vert ^d$, with $d$ being the diameter\footnote{The diameter $d$ of a graph $G$ is the maximum distance between any two nodes of the graph. The distance between two nodes is the number of edges in the shortest path that connects the two.} of the entity graph. Indeed, every node of the graph can be associated with every types. Therefore when applying the \emph{unique type} summarisation relation $R_{ut}$ over such a graph, we need for each path in $G$ to create an edge to each type for every node of the path. Figure~\ref{fig:unique-type-summary-size} depicts the size of the \emph{unique type} summary in the worst case scenario, where all nodes are both of types $A$ and $B$.

	The summary is sensitive to heterogeneous entity graph. Indeed, there can be as many sumnodes in an attributes summary $\mathcal{S}_a = \left\langle \mathcal{W}_{a}, \mathcal{B}_{a}, l_{\mathcal{W}_{a}} \right\rangle$ as there is elements in the attribute power-set minus the empty set, i.e., $\vert \mathcal{W}_a \vert = \vert \mathbb{P} \left( \{ \alpha \mid \forall (u, v) \in \mathcal{W}_a\; (u, \alpha, v) \in \mathcal{W}_a \} \right) \vert - 1$. Such an order of the summary can be expected for any summarisation relation based on the set of attributes, i.e., the summarisation relations $R_{at}$, $R_{ioa}$, $R_{ioat}$, and $R_{fbt}$.\\

	We measure the amount of information from a given graph $G_1 = \left\langle V_1, A_1, l_{V_1} \right\rangle$ that is compressed into another graph $G_2 = \left\langle V_2, A_2, l_{V_2} \right\rangle$. To this end, we compare the size and order of $G_2$ against the size and order of $G_1$. We report this comparison as the ratio $G_2:G_1$:

	$$
	G_2:G_1 = \frac{\vert V_2 \vert + \vert A_2 \vert}{\vert V_1 \vert + \vert A_1 \vert}
	$$

	\item[\emph{Algorithm performance.}]

	%The MapReduce implementation of the graph summarisation is composed of two separate steps:
	%\begin{inparaenum}[(1)]
	%\item a \emph{mapping} step, where we assign a node of the entity graph to its $\sim$-equivalence class; and
	%\item a \emph{edges} step, where we compute the edges of $G^\sim$.
	%\end{inparaenum}
	We evaluate the computational performance of a summarisation relation by analysing the CPU time\footnote{The accumulated CPU time as reported with the \texttt{CPU\_MILLISECONDS} counter of Hadoop.} on the \hyperref[step-he]{Step 3} of the graph summary computation, as described in Section~\ref{chap03:algo:edge-materialisation} for the MapReduce case. We do not report on the \hyperref[step-hn]{Step 2} because the evaluated summarisation relations have the same complexity, achieving similar times on this step.

	\item[\emph{Summary precision.}]

	With regards to all three classification of errors, we evaluate the precision of a summary thanks to the true and false positive sumedges set $TP(x)$ and $FP(x)$.
	We report the precision using two measures, i.e., $P1$ and $P2$.

	The measure $P1$ reflects the average number of true positive sumedges from a randomly selected node in the inferred graph $\mathcal{I}(R)$. In the equation below of $P1$, from right to left, we sum the sumedge precisions $Prec(R, x)$ of a node $x\in \mathcal{W}_{fbt}$, which we average by the number of nodes within $C(c)$. Given that there exists a relation $S \subseteq \mathcal{W}_{fbt} \times \mathcal{W}$ such that $R_{fbt} \sqsubseteq R$, the expression $C(c)$ is the set of sumnodes in $\mathcal{W}_{fbt}$ that are mapped to the sumnode $c \in \mathcal{W}$ as per the relation $S$. Finally, we average over the total number of nodes within the bisimulation summary.
	$$
	\begin{aligned}
	P1 = & \frac{1}{\vert \mathcal{W}_{fbt} \vert} \times \sum_{c \in \mathcal{W}}{\frac{1}{\vert C(c) \vert} \times \sum_{x \in C(c)}{Prec(R, x)}} \\
	\text{where}\; C(c) = & \left\lbrace x \in \mathcal{W}_{fbt} \mid (x, c) \in S \right\rbrace
	\end{aligned}
	$$

	The measure $P2$ reflects the overall chance to randomly select a true positive sumedge in the inferred graph $\mathcal{I}(R)$:
	$$
	P2 = \frac{\sum_{x \in \mathcal{W}_{fbt}}{TP(x)}}{\sum_{x \in \mathcal{W}_{fbt}}{TP(x) + FP(x)}}
	$$
\end{quotation}

\begin{figure}
	\centering
	\resizebox{.8\textwidth}{!}{
		\input{04-summary-precision/figures/unique-type-summary-size}
	}
	\caption{Size of the \emph{unique type} summary $\mathcal{S}_{ut}$}
	\label{fig:unique-type-summary-size}
\end{figure}
\subsection{Datasets}
\label{sec:eval:datasets}

We use in this evaluation several datasets of varying complexity. We list in the Table~\ref{tab:datasets} the datasets along with some descriptive statistics. The datasets are grouped according to their complexity into four categories, i.e., \emph{Low}, \emph{Medium}, \emph{High}, and \emph{Very High}. We determine the complexity with regards to the number of nodes and edges in a graph, as well as with the number of unique types and attributes it possesses.

For each dataset, we present two aspects, i.e., the \emph{schema} and the \emph{structure} of the entity graph. With regards to the schema complexity, we report the number of unique edge labels and types of the entity graph. With regards to the structure complexity, we report the size and order of $G$ and  $\mathcal{S}_{fb}$. The order values omit the number of sink nodes, i.e., nodes without outgoing edges which includes literal nodes in the RDF data model.

The $G:\mathcal{S}_{fb}$ column reports the compression ratio of the graph $G$ when the bisimulation summarisation is used. We remark that the size and order of the bisimulation summary $\mathcal{S}_{fb}$ is significantly smaller than of the entity graph. This emphasize the performance benefits of using a summary instead of the entity graph itself in an application. We note the lower the compression ratio, the more complex the graph is to summarise. Although the datasets \texttt{bnb} and \texttt{wb} have a similar size and order, the compression ratio of the summary on \texttt{bnb} is greater than for \texttt{wb}, i.e., $85.43$ to $13125.89$. This shows that the dataset \texttt{bnb} has a more heterogeneous structure than that of \texttt{wb}.

\input{04-summary/experiments/datasets}

\subsection{Results}
\label{sec:eval:results}

We evaluate and compare in this section the different graph summarisation relations according to the compression ratio, the computational complexity, and the precision of the generated summary. Then, we discuss the trade-offs with respect to these three dimensions.
We note the mean of measurements in a category, i.e., \emph{Low}, \emph{Medium}, \emph{High}, and \emph{Very High}, as $\mu_{L}$, $\mu_{M}$, $\mu_{H}$, and $\mu_{VH}$, respectively.

\subsubsection{Summary Compression}

We depict in Figure~\ref{fig:vol-ration-bar} the average compression ratio in a log scale in each category of complexity for every summarisation relation. The raw data is reported in Table~\ref{tab:volume-ratio}, in the Appendix section.

The summaries based only on the type information, i.e., the \emph{unique type} summary $\mathcal{S}_{ut}$ and the types summary $\mathcal{S}_t$, exhibit a higher compression compared to that of the gold standard, i.e., $G:\mathcal{S}_{fbt}$. The compression ratio is twice as much on the \emph{Low} and \emph{Medium} datasets and slightly higher on the more complex datasets. On average, the types $\mathcal{S}_t$ and  \emph{unique type} $\mathcal{S}_{ut}$ summaries present a similar compression ratio.
%we note that the compression ratio of a  is higher than with the types summary $\mathcal{S}_t$.% The reason is that a node of the entity graph can be mapped to several sumnodes with the \emph{Unique Type} summary $R_{ut}$, which leads to an increase of the size of the summary.

The attribute feature appears to be a better feature for summarising a graph than type. Indeed, the compression ratio of the attributes  $\mathcal{S}_a$ and IO attributes $\mathcal{S}_{ioa}$ summaries remain stable across the dataset categories. However, this is not the case with the summaries based only on the type, i.e., the $\mathcal{S}_{ut}$ and $\mathcal{S}_t$ summaries.
% $\mathcal{S}_a$ and $\mathcal{S}_{ioa}$ remains stable with \emph{Medium} ($42.51$ and $51.45$, resp.) and \emph{High} ($47.99$ and $66.13$, resp.) datasets. However, this is not the case with the $\mathcal{S}_{ut}$ and $\mathcal{S}_t$ summaries.

We can observe a correlation between the compression ratio and the number of types.
We note that the \emph{Attributes} summarisation $R_a$ actually achieves the lowest ratio on the \emph{very high} dataset. This shows that the use of attributes in \texttt{dbpedia} is homogeneous across types. The compression ratio of the \emph{IO Attributes} summary $\mathcal{S}_{ioa}$ is on average on par with the ratio of the \emph{Attributes} summary $\mathcal{S}_a$, indicating a certain homogeneity of incoming edges in the entity graph.

By using both type and attribute information as with the \emph{Attributes \& Types} summarisation relation $R_{at}$, we remark that this can lead to a significant decrease of the summary's compression, as reported on the \emph{Medium} and \emph{High} categories.
Indeed, the combination of the features on the \emph{High} datasets exhibits a much higher ratio than when taken separately.%, e.g., $55.87$ and $84.06$ respectively with $\mathcal{S}_{at}$, against $27.43$ and $49.83$ with $\mathcal{S}_a$.
We can conclude that a sparse usage of attributes with a type creates an explosion of combinations. In general, the type feature is less stable than the attribute feature.

\input{04-summary-precision/exps/compression-ratio}

\subsubsection{Performance of the Summarisation Relation}

Figure~\ref{fig:cpu-time-bar} depicts the average CPU time in $ms$ for each category of complexity. This time accounts for the \hyperref[step-he]{Step~3} (Section~\ref{chap03:algo:edge-materialisation}) in the graph summarisation computation.
For each dataset which raw data is reported in Table~\ref{tab:cpu-time}, the times are the average of two runs of a summarisation relation.

The \emph{unique type} $R_{ut}$ and types $R_t$ summarisation relations can have an undefined sumnode (Section~\ref{sec:undefined-sumnode}) due to some nodes having no type information. For performance reason, the undefined sumnode $\mathfrak{U}$ is filtered from the resulting summary. Therefore, reported run times in Table~\ref{tab:cpu-time} from $R_{ut}$ and $R_t$ cannot be compared to others.

On the \emph{Medium}, \emph{High}, and \emph{Very High} datasets, the time taken by the $R_{ut}$ summarisation on the \hyperref[step-he]{Step~3} of the graph summary computation is higher than $R_t$. This highlights the property of the $R_{ut}$ summarisation of being a many-to-many binary relation rather than a many-to-one, i.e., it can assign a node to several sumnodes. As a consequence, we need to compute all the possible combinations of edges between the sumnodes.
We note that the incoming attribute feature in $R_{ioa}$ and $R_{ioat}$ does not imply a higher runtime when compared to $R_{a}$ and $R_{at}$.

\input{04-summary-precision/exps/cpu-time}

\subsubsection{Graph Summary Precision}

We discuss in this section the precision of a summary with regards to the connectivity first, then to the type and attribute next. We do not report the precision in any error classification for the \texttt{dbpedia} dataset (Very High category). The reason is we were unable to evaluate the precision on it due to performance issues. While the performance evaluation did not account for the undefined sumnode $\mathfrak{U}$, we do consider it for the precision evaluation.

\minisec{Connectivity Precision}

Figure~\ref{fig:link-precision-bar} depicts the average connectivity precision for $P1$ and $P2$ over three categories of complexity, i.e., \emph{Low}, \emph{Medium}, and \emph{High}. The raw results for each dataset are reported in Table~\ref{tab:precision-conn} of the Appendix section; for each category of dataset complexity, we report as well the mean $\mu$ of the connectivity precision.\\

We discuss first the connectivity error with regards to the precision $P1$ plotted on Figure~\ref{fig:err-conn-p1}.
The summarisation relations based only on the type feature, i.e., $R_{ut}$ and $R_t$, provide a low connectivity precision. Indeed, they show on average a connectivity precision of $25\%$ according to $P1$, i.e., $\mu_H=0.2414$ for the types summarisation relation $R_t$.

Summarisation based on the attribute feature only, i.e., the attributes summarisation relation $R_a$ , provide also a low precision on \emph{Medium} and \emph{High} categories. On the \emph{Low} category, the attribute feature exhibits a better precision than the type feature, i.e., $\mu_L=0.5579$ for $R_a$ against $\mu_H=0.3617$ for $R_{ut}$. However, when the type and attribute features are combined in the attributes \$ types summarisation relation $R_{at}$, this provides a significant increase of the precision across all three categories. According to $P1$, we reach on average a $50\%$ connectivity precision ($0.5124$) on the \emph{High} category for $R_{at}$, and at least $20\%$ on \emph{Medium}.

We remark that the incoming attribute in the IO attributes summarisation relation $R_{ioa}$ is an important feature that increases the precision. The $R_{ioa}$ summarisation provides a precision of $30\%$ on the \emph{Medium} up to $50\%$ on \emph{High}, whereas $R_a$ reaches $15\%$ on \emph{Medium} and $10\%$ on \emph{High}. Overall, we can achieve a good average connectivity precision with $R_{ioat}$ according to $P1$ when we add the type feature.\\

We remark that the overall precision $P2$ is very low on the datasets of \emph{Medium} and \emph{High} complexities. This suggests that few nodes of the summary have a high out-degree, creating a combinatorial explosion of false positive edges. This will be investigated in future work.

\input{04-summary-precision/exps/connectivity-precision}

\minisec{Schema Precision}

Figure~\ref{fig:schema-precision-bar} depicts the average type and attribute precisions over the categories of complexity.
In red we represent the attribute schema error, and in blue the type schema error. Bars with north east lines pattern depict $\mu_{L}$, with dotted pattern $\mu_{M}$, and with north west lines $\mu_{H}$.
The raw data is reported in Table~\ref{tab:precision-schema} of the Appendix section.

The summarisation relations $R_{ut}$ and $R_t$ based on the type feature provide an attribute precision above at least $60\%$ for $P1$ ($\mu_M=0.5927$ for $\sim_{ut}$).
On the contrary, the attribute feature reports a good type precision, reaching on average at least $90\%$ for $R_a$, i.e., $P1=0.9222$ on the \emph{Medium} datasets.

Incoming attributes do not increase much the type precision, since the type precision of $R_a$ stays on par with $R_{ioa}$. The $R_{at}$ relation provides a perfect summarisation of the graph schema. Again, the significant differences between $P1$ and $P2$ suggests one more time that few nodes of the summary contains a high out-degree, creating a combinatorial explosion of false positive edges.

\input{04-summary-precision/exps/schema-precision}

\minisec{Discussion}

In conclusion, we observe that a combination of both type and attribute features is necessary to achieve a good precision. The results show that taking incoming attributes as a feature of the summarisation relation is important for the connectivity precision, but not for the schema.

However, there is place for improvement for overall connectivity precision especially on certain datasets. We observe that the precision $P2$ leads to very low precision values which is caused by a few summary nodes with a high out-degree. This indicates that the model of $P2$ is not appropriate for measuring the precision of a summary in terms of connectivity and schema.

\subsubsection{Trade-Offs}

We report in Figure~\ref{fig:trade-conn-volume} the trade-off between the average connectivity precision and the average compression ratio across all datasets among all the relations. We can distinguish two groups of relations, the type-based summarisation relations, i.e., the types $R_t$ and the \emph{unique type} $R_{ut}$ summarisation relation, and the attribute-based ones, i.e., the attributes relation $R_a$, the attributes \& types relation $R_{at}$, the IO attributes relation $R_{ioa}$, and the IO attributes \& types relation $R_{ioat}$.\\

The type-based relations provide the best compression ratio, but also the worst precision. In the attribute group, the compression ratio among relations is close to each others, but their precision differs greatly, with $R_{ioat}$ ahead.

This suggests that in terms of trade-off between connectivity precision and compression ratio, $R_{ioat}$ is the best candidate.\\

We report in Figure~\ref{fig:trade-schema-volume} the trade-off between the average schema precision and the average compression ratio across all datasets among all the relations. Again we can distinguish the same two groups. However, in the attribute group, the precision does not differ as much among the candidates, each one being either equal or very close to $1$. In the type group, the \emph{Types} summarisation relation $R_t$ provides a reasonable precision for a good compression ratio.

This suggests that if the precision is primordial, the \emph{Attributes \& Types} relation $R_{at}$ is the best candidate, providing a perfect schema precision for the best compression ratio. However, if the compression is primordial instead and that some imperfection can be tolerated, then the \emph{Types} summarisation relation $R_t$ is the best candidate.\\

We report in Figure~\ref{fig:trade-conn-cpu} (respectively, \ref{fig:trade-schema-cpu}) the trade-off between the average connectivity precision (respectively, average schema precision) and the average CPU time across all datasets among all the summarisation relations. Among the attribute-based algorithms $R_a$, $R_{at}$, $R_{ioa}$ and $R_{ioat}$, the latter is the one that achieves the best runtime with the highest precision. Among the type-based algorithms, the \emph{Types} summarisation $R_t$ achieves a lower runtime and a higher precision than the \emph{Unique Type} relation $R_{ut}$.

If the schema precision is primordial and a low connectivity precision can be tolerated, the \emph{Types} summarisation $R_t$ is the best candidate as it provides a high schema precision, with the best CPU time. On the contrary, if the connectivity is primordial, the summarisation relation $R_{ioat}$ is the best candidate, but this at the cost of a longer runtime.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.46\textwidth}
		\resizebox{\textwidth}{!}{
			\input{04-summary/experiments/tradeoff-volume*conn}
		}
		\caption{Connectivity precision versus compression ratio.}
		\label{fig:trade-conn-volume}
	\end{subfigure}
	\qquad
	\begin{subfigure}[t]{0.46\textwidth}
		\resizebox{\textwidth}{!}{
			\input{04-summary/experiments/tradeoff-volume*schema}
		}
		\caption{Schema precision versus compression ratio.}
		\label{fig:trade-schema-volume}
	\end{subfigure}
	\qquad%
	\begin{subfigure}[t]{0.46\textwidth}
		\resizebox{\textwidth}{!}{
			\input{04-summary/experiments/tradeoff-cpu*conn}
		}
		\caption{Connectivity precision versus summarisation performance.}
		\label{fig:trade-conn-cpu}
	\end{subfigure}
	\qquad%
	\begin{subfigure}[t]{0.46\textwidth}
		\resizebox{\textwidth}{!}{
			\input{04-summary/experiments/tradeoff-cpu*schema}
		}
		\caption{Schema precision versus summarisation performance.}
		\label{fig:trade-schema-cpu}
	\end{subfigure}
	\caption{Efficiency and precision trade-offs of the candidate summarisation relations. The values are taken as the average across all dataset categories.}
\end{figure}
