\section{MF Ranking Model}
\label{chap:tree-ranking:mf-model}

In general, an entity contains some structure which provides relevant information about its content. For example, an entity that describes a book would be divided into chapters, each chapter into sections, each section into a set of paragraphs. In order to retain this structure for later use in the ranking, we generalize field-based approaches with the ``MF'' ranking model for Directed Acyclic Graphs (DAG) by integrating the complete graph structure into its model.

First, we present the revised entity model and introduce the formal model of MF. Next we present two extensions to the MF model. Finally, we describe weights developed for the MF model.

\subsection{Model}

In traditional field-based ranking models, an entity is modelled as a set of fields, each being a \emph{bag of words}. However in doing so, the rich structure of the entity is lost. For example, in Figure~\ref{fig:concept-tree} where the graph of an entity rooted in $N_0$ is depicted, we can create three fields, one for each attribute $a$, $b$, and $c$; the label of nodes are then distributed as follows:
\begin{itemize}
	\item $N_1$ is assigned to field $a$;
	\item $N_2$, $N_4$, and $N_5$ are assigned to field $b$; and
	\item $N_3$ and $N_4$ are assigned to the field $c$.
\end{itemize}
However, a shortcoming is that the relationship between nodes is discarded within a field. Indeed, in traditional field-based ranking models, most of the entity's structure is discarded.\\

Rather that representing the entity as a set of fields, the MF ranking model follows the structure of the graph. Each node of the graph --- not a set of nodes --- is represented as a bag of words. This allows to capture the graph's structure into the ranking model.

\paragraph{Expanded entity graph.}

Our model revolves around two core operations: on the one hand, the importance of an entity with regards to a query is computed on the leaves; on the other hand, internal nodes are used for aggregating the scores. In order to take into account
\begin{inparaenum}[(a)]
	\item the label of internal nodes; and
	\item the label of edges,
\end{inparaenum}
we expand the graph so that each label becomes a leaf. To do so, we transform the entity graph into a graph where each label --- edge and node --- is a node, as depicted in Figure~\ref{fig:expanded-graph1}. Then, each node of that transformed graph becomes a leaf of the tree, as depicted in Figure~\ref{fig:expanded-graph2}; this is the expanded entity graph of the entity in Figure~\ref{fig:concept-tree}.

\begin{figure}
	\centering
	\begin{subfigure}{.3\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/tikz/concept-tree}
		}
		\caption{An entity description represented as a DAG. The root of the entity is the node $N_0$}
		\label{fig:concept-tree}
	\end{subfigure}
	\quad
	\begin{subfigure}{.3\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/tikz/expanded-graph1}
		}
		\caption{The graph of the entity in Figure~\ref{fig:concept-tree} where each label --- of a node and an edge alike --- becomes a node}
		\label{fig:expanded-graph1}
	\end{subfigure}
	\qquad
	\begin{subfigure}{.7\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/tikz/expanded-graph2}
		}
		\caption{The expanded graph of the entity in Figure~\ref{fig:concept-tree} where each node of the graph in Figure~\ref{fig:expanded-graph1} becomes a leaf. Nodes labelled with $*$ depict aggregations.}
		\label{fig:expanded-graph2}
	\end{subfigure}
\end{figure}

\paragraph{Algorithm.}

The score of an entity with regards to a query is computed with a bottom-up approach. The leaves of the entity's DAG contain information about the entity; a function that leverage the content and which returns a numerical value is then applied on that level. The internal nodes of the DAG aggregates the values computed on its outgoing nodes using a second function; the aggregation continues until the root of the entity is reached.

For instance, we compute the contribution of the leaves ``$d$'', ``$N_5$'', and ``$N_4$'', which are aggregated on the parent node. The aggregated value is then combined with its sibling nodes, i.e., ``$b$'' and ``$N_2$''. These operations are repeated until the root of the tree is reached.

\paragraph{MF Model Normalisations}
\label{chap:tree-ranking:mf-model:norm}

Ranking models use statistics garnered from the entity to estimate its relevance with regards to a query. For instance, the frequency of a term can be used as an indicator for its importance in describing the entity. However, the entity is part of a dataset and the significance of the frequency as a measure varies between entities. This variability may due to several factors, e.g., the writing style; two entities can provide the same amount of information about a certain topic, but one uses twice as much terms as the other.

In order to account for this variability in the dataset, there is a need for \emph{normalising}~\cite{manning:2008:iir} the statistics garnered from the entity. For instance, in BM25F~\cite{zaragoza:2004:microsoft} the frequency of a term within a field is normalised based on
\begin{inparaenum}[(a)]
	\item the length of the field; and
	\item the average length of the field within the dataset.
\end{inparaenum}
Thanks to the normalisation, it is possible to compare the relevance with regards to a query of entities within a dataset.\\

With traditional field-based ranking models, there is one level of normalisation that is performed on the content level, as discussed in the previous paragraph.
The MF ranking model offers two levels of normalisations:
\begin{enumerate}
	\item a normalisation on the \emph{content} level to account for the variability of a node's length within a dataset; and
	\label{norm:content}
	\item a normalisation of the outgoing \emph{degree} of a node.
	\label{norm:degree}
\end{enumerate}
The first normalisation is applied on the \emph{leaf} nodes of the DAG. The second normalisation is applied on the \emph{internal} nodes during the aggregation step. This additional level of normalisation allows a finer tuning of the ranking function.

%\begin{figure}[t]
%    \centering
%%    \begin{subfigure}{0.3\textwidth}
%%    	\centering
%%      	\input{05-ranking/figures/tikz/concept-tree}
%%    	\caption{An entity represented as a tree with varying degree.}
%%		\label{fig:concept-tree}
%%    \end{subfigure}
%%  	\quad
%  	\begin{subfigure}{0.3\textwidth}
%  		\centering
%  		\input{05-ranking/figures/tikz/field-model}
%  		\caption{Field-based ranking model for the entity. The $*$ symbolises the aggregation of the contribution of each child node, here a field.}
%		\label{fig:field-model}
%  	\end{subfigure}
%  	\quad
%  	\begin{subfigure}{0.3\textwidth}
%  		\centering
%  		\input{05-ranking/figures/tikz/mf-model}
%  		\caption{The MF ranking model for the entity. A square symbolizes where the contribution of a query term is performed. Red dashed squares represents internal nodes in the entity tree that were transformed into a leaf.}
%		\label{fig:mf-model}
%  	\end{subfigure}
%  	\quad
%  	\begin{subfigure}{\textwidth}
%  		\centering
%	  	\input{05-ranking/figures/tikz/mf-ranking}
%  		\caption{Normalized term frequency computation in the MF ranking for the entity.}
%  		\label{fig:mf-ranking}
%  	\end{subfigure}
%	\caption{Abstract models for ranking a tree.}
%	\label{fig:tree}
%\end{figure}

\paragraph{Eliteness.}

In \cite{harter:1974:thesis}, Harter introduced the notion of \emph{eliteness} in order to model content-bearing terms: a document is said to be \emph{elite} in term $t$ if it is somehow ``about'' the topic associated with $t$. In \cite{robertson:1981:PMI}, Robertson et al. introduce the relationship between the eliteness of a term in a document and its frequency: an elite term is most likely to be reused in the document, hence the term frequency is used as evidence of the term eliteness in the document.

In \cite{zaragoza:2004:microsoft}, Zaragoza et al. extend the notion of eliteness to documents with multiple fields. In \cite{robertson:2004:cikm}, the authors argue that the normalized frequencies of a term in each field should be combined before applying the term weighting model.

Similarly in our MF model, the term eliteness in an entity is shared between its attributes. The values related to a same attribute are associated to a same topic, described by the edge label. Therefore, a term eliteness in an attribute is shared between its values.\\

Although values are related to a same attribute, the relevancy of each value with regard to the query is different. We can assume that, given a same attribute, an entity where two terms occur in a single value is more relevant than another entity where each term occurs in two values. This can be seen as a way to integrate some kind of term proximity measure in the retrieval model, i.e., two words are considered close to each other when they occur in the same value. Reflecting this difference into the importance of a term using an appropriate value-specific weight can improve the ranking efficiency.

\subsection{MF Ranking Functions}
\label{sec:mf-function}

In this section, we describe \emph{BM25MF} and \emph{PL2MF}, the MF extensions of BM25F~\cite{zaragoza:2004:microsoft} and PL2F~\cite{macdonald:2005:clef}, respectively.
We present first the features needed for the MF ranking model and then define both extensions.

\paragraph{Ranking features.}

The MF ranking model requires the following features:
\begin{labeling}{\textbf{average attribute cardinality:}}
  \item[\textbf{leaf length:}] refers to the number of terms in the label of a leaf;
  \item[\textbf{average leaf length:}] is the mean of the \emph{leaf length} of sibling nodes;
  \item[\textbf{attribute cardinality:}] is equal to the number of values an attribute possesses;
  \item[\textbf{average attribute cardinality:}] is equal to the mean of the \emph{attribute cardinality} across the entities where that attribute appears.
\end{labeling}

\subsection{BM25MF}

BM25F is extended by adapting the Equation (\ref{eq:bm25f_1}). The resulting normalised term frequency is then passed to the saturation function in the Equation (\ref{eq:bm25f_2}).
The computation of the entity score with BM25MF consists of applying two functions, $f_{t,L}$ and $f_{t,I}$, applied on a leaf and on an internal node, respectively.

The BM25MF ranking function is defined following the two levels of normalisations.
The $f_{t,L}$ function implements the Normalisation~\ref{norm:content} on the content and is defined as follows:
$$
\label{bm25mf_v}
f_{t,L} = \frac{\alpha_v\times F_t}{1+b_v\times\left(\frac{l}{l_a}-1\right)}
$$
in which the following notations are used:
\begin{itemize}
	\item $F_t$ is the frequency of the term $t$ within the leaf;
	\item $l$ is the \emph{leaf length} of the leaf; and
	\item $l_a$ is the \emph{average leaf length}.
	\item $\alpha_v$ is a weight specific to a leaf; and
	\item $b_v$ where $b_v \in \left[0,1\right]$ is a parameter of the Normalization~\ref{norm:content}.
\end{itemize}

After applying the $f_{t,L}$ function, results from the leaves are summed up on the parent nodes. Then, a parent node applies the function $f_{t,I}$ over the sum. This process is repeated until the root of the DAG is reached. The $f_{t,I}$ function implements the Normalisation~\ref{norm:degree} on the degree of an internal node. This function is defined as follows:
$$
\label{bm25mf_a}
f_{t,I} = \frac{\alpha_a \times \sum{f_{t,L}} }{ 1 + b_a \times \left(\frac{ \left|{a}\right|_e }{ \left|{a}\right| } - 1\right) }
$$
in which the following notations are used:
\begin{itemize}
\item $\left|{a}\right|_e$ is the \emph{attribute cardinality} of the attribute $a$ in the entity $e$;
\item $\left|{a}\right|$ is the \emph{average attribute cardinality} of the attribute $a$;
\item $\alpha_a$ is a weight specific to an internal node; and
\item $b_a$ where $b_a \in \left[0,1\right]$ is a parameter of the Normalization~\ref{norm:degree}.
\end{itemize}

If we consider the Figure~\ref{fig:expanded-graph2} for instance, we apply the Equation~(\ref{bm25mf_v}) over the leaves $d$, $N_5$, and $N_4$. On the parent node, results of the function $f_{t,L}$ on the three previous leaves are summed up and passed to the Equation~(\ref{bm25mf_a}).

\subsection{PL2MF}

PL2F is extended by adapting the Equation (\ref{eq:pl2f}). The resulting normalised term frequency is then passed to the term frequency normalization function (\ref{eq:dfr-prisk}).
The computation of the entity score with PL2MF consists of applying two functions, $f_{t,l}$ and $f_t$, applied on a leaf and on an internal node, respectively. After applying $f_{t,l}$ on the leaves, the results are summed up on the parent node. Then, this parent node applies the function $f_t$ on the result of the sum. This process is repeated until the root of the tree is reached.
\begin{eqnarray}
  \label{eq:pl2mf_v}
  tfn_a & = & \sum_{v\in a}{\alpha_v\times f_{t,e,v} \times log_2\left(1+c_v\times\frac{l_a}{l_{e,v}}\right)}\\
  \label{eq:pl2mf_a}
  tfn & = & \sum_{a\in e}{\alpha_a\times tfn_a \times log_2\left(1+c_a\times\frac{\left|{a}\right|}{\left|{a}\right|_e}\right)}
\end{eqnarray}
where $c_a$ and $c_v$ are hyperparameters, with $c_a$ specific to the attribute $a$ and $c_v$ to the value $v$, with $(c_a,c_v) \in\; ]0,+\infty[^2$.

\subsubsection{Generalisation}

In Equations (\ref{bm25mf_v}) and (\ref{eq:pl2mf_v}), we normalize the term frequency based on the \emph{average field length} $l_a$. In Equations (\ref{bm25mf_a}) and (\ref{eq:pl2mf_a}), we further normalize the term frequency based on the \emph{average attribute cardinality} $\left|{a}\right|$.
In addition to attribute-specific weights $\alpha_a$, the MF ranking model allows value-specific weights in its implementations with the parameter $\alpha_v$. We will present value and attribute specific weights in the next section.

If we assume a single value per attribute to match field-based ranking models, then the Equations (\ref{bm25mf_v}) and (\ref{bm25mf_a}) are transformed into the Equation (\ref{eq:bm25f_2}), with $\alpha_a\times\alpha_v$ as the BM25F's attribute weight, and $b_v$ as the attribute normalization parameter. BM25MF is under this condition equivalent to BM25F. Under the same assumption, the Equations (\ref{eq:pl2mf_v}) and (\ref{eq:pl2mf_a}) are transformed into the Equation (\ref{eq:pl2f}), with $\alpha_a\times\alpha_v\times log_2(1+c_a)$ as the PL2F's attribute weight. PL2MF is under this condition equivalent to PL2F. Therefore, the MF model is a generalisation of field-based models for semi-structured data with multi-valued attributes.
