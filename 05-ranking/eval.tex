\section{Experiments}
\label{sec:experiments}

In order to evaluate the MF model, we perform several experiments using three different datasets. We start by experimenting on the normalisation parameters in order to study their impact on the effectiveness of the approach. We then compare the MF ranking functions against other traditional ones. We finally discuss the consequence of \textbf{not} considering the attribute label as a source of potential relevant terms and demonstrate the effectiveness of the proposed weights.

While there are other ways to perform entity ranking on the Web of Data, e.g., one can look at the other SemSearch 2011 candidates, in this evaluation we concentrate on demonstrating how the MF model specifically extends and improves the very popular PRF and DFR frameworks.
%\footnote{SemSearch2011: \url{http://km.aifb.kit.edu/ws/semsearch11}}

\subsection{Datasets}
\label{sec:datasets}

The datasets we are using in our experiments are the following:
\begin{description}
  \item[INEX09] a dataset of 2,491,134 triples from DBpedia containing the description of entities in English, and converted for the INEX evaluation framework~\cite{Perez-Aguera:2010:UBS};
  \item[SS10] the ``Billion Triple Challenge''\footnote{Billion Triple Challenge: \url{http://vmlion25.deri.ie}} (BTC) dataset, containing more than one billion triples, with the assessments (a file containing relevance judgments on a sample of query answers) of the SemSearch2010\footnote{SemSearch2010: \url{http://km.aifb.kit.edu/ws/semsearch10}} challenge;
  \item[SS11] the BTC dataset with the assessments of the ``Entity Search track'' of the SemSearch2011\footnote{SemSearch2011: \url{http://km.aifb.kit.edu/ws/semsearch11/}} challenge.
\end{description}
The INEX09 dataset is significantly different than the other two based on BTC. Indeed, BTC is a heterogeneous dataset, created from web crawls of several search engines. INEX09 is a highly curated dataset from DBpedia.

%\subsection{Effectiveness of the MF Ranking Model}
%\label{sec:mv-fields-effectiveness}
%
% Next we perform a comparison between the MF extensions, i.e., BM25MF and PL2MF, against other ranking approaches.

\subsection{\emph{The Normalisation Parameters}}
\label{sec:norm-exp}

In this section, we study the impact on the ranking of the normalisation parameters. In addition to the length normalisation of field-based ranking function (Normalisation~\ref{norm:content}), the MF ranking model offers an additional normalisation on the attribute's cardinality (Normalisation~\ref{norm:degree}). The effectiveness of the PRF and DFR frameworks depends on finding the right values for the normalisation parameters. However, these parameters are highly dependent on the dataset.\\

Figures~\ref{fig:bm25mf-norm} and~\ref{fig:pl2mf-norm} depict the impact of the normalisation parameters on the ranking performance of BM25MF and PL2MF, respectively. For all three datasets, the figures depict on the \textbf{Y} axis the Mean Average Precision (MAP) scores, and on the \textbf{X} axis the Normalisation~\ref{norm:content}, i.e., with parameter $b_v$ in Figure~\ref{fig:bm25mf-norm} (resp., parameter $c_v$ in Figure~\ref{fig:pl2mf-norm}). Each curve plots the results with a fixed Normalisation~\ref{norm:degree} parameter, i.e., $b_a$ in Figure~\ref{fig:bm25mf-norm} and $c_a$ in Figure~\ref{fig:pl2mf-norm}.

The grid of parameters values in Figure~\ref{fig:bm25mf-norm} ranges from $0$ to $1$ with a step of $0.1$. In Figure~\ref{fig:pl2mf-norm}, the grid ranges from $0.5$ to $10.5$ with a step of $1$. Although these parameters can be attribute and value-specific, this experiment considers a constant parameter in order to reduce the number of combinations and to lower the variability of the results. Dashed lines depict the MAP scores of BM25F and PL2F and solid lines the scores of their MF extension, BM25MF and PL2MF respectively.\\

These plots show that Normalisation~\ref{norm:degree}, the normalisation introduced with the MF ranking model, improves the performance. Indeed, the Normalisation~\ref{norm:content} parameters $b_v$ and $c_v$, which are traditional to the field-based ranking model, do not grasp completely the heterogeneity in the data, which results in lower performance when compared to the MF extensions. This indicates that the distinction between leaves within an attribute has a positive effect on the ranking.

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\input{05-ranking/figures/tikz/norm-exp-bm25mf}
		\addtocounter{subfigure}{-1}
		\caption{Evaluation of BM25MF normalisation parameters. A curve plots a fixed $b_v$ value (Equation~(\ref{bm25mf_v})) with $b_a$ (Equation~(\ref{bm25mf_a})) varying from $0$ to $1$ with a precision step of $0.1$.}
		\label{fig:bm25mf-norm}
	\end{subfigure}
	\qquad
	\begin{subfigure}{\textwidth}
		\centering
		\input{05-ranking/figures/tikz/norm-exp-pl2mf}
		\addtocounter{subfigure}{-1}
		\caption{Evaluation of PL2MF normalisation parameters. A curve plots a fixed $c_v$ value (Equation~(\ref{eq:pl2mf_v})) with $c_a$ (Equation~(\ref{eq:pl2mf_a})) varying from $0.5$ to $10.5$ with a precision step of $1$.}
		\label{fig:pl2mf-norm}
	\end{subfigure}
	\caption[Impact of the normalisation parameters on the MF ranking functions]{Impact of the normalisation parameters on the MF ranking functions. The figures report the MAP values of the respective datasets on the Y axis. On the X axis and with each curve, we vary the values of normalisation parameters.}
\end{figure}

\subsection{Comparison between MF and Field-Based Models}
\label{sec:mf-field-cmp}

In this section, we evaluate and compare the performance of the BM25 and PL2 ranking models against their MF extensions, BM25MF and PL2F respectively, and show the superiority of the MF model. The TF-IDF function is used as a baseline.

We outline below the ranking functions that are used in the experiments. We reuse the terminology for the equations that was presented in the previous section.

\begin{labeling}{\emph{BM25~\cite{robertson:1994:sigir}}}
  \item[\emph{TF-IDF}] is a logarithmic function of the term frequency and defines the Equation (\ref{eq:tfidf-score}) as
  $
  tfn_e=log(F_t)+1
  $,
  where $F_t$ is the number of occurrences of the term $t$ in the entity.
  \item[\emph{BM25~\cite{robertson:1994:sigir}}] considers the document as a simple bag of words. It is a function of the term frequency derived from a two-Poisson model and it uses an entity-length normalisation. The entity length is computed as the sum of the \emph{field length} defined in the Section~\ref{sec:ranking-wod}.

  It defines the Equation (\ref{eq:tfidf-score}) as
  $
  tfn_e=\frac{F_t\times(k_1+1)}{F_t+k_1\times \left(1+b\times\left(\frac{l_e}{l_{avg}}-1\right)\right)}
  $,
where $l_e$ is the \emph{entity length} of the entity $e$, $l_{avg}$ is the average of the \emph{entity length} in the collection and $b$ is a normalisation parameter.
  \item[\emph{BM25F}] is defined in Equation (\ref{eq:tfidf-score}). It considers documents as composed of fields, each field being a bag of words.
  \item[\emph{PL2~\cite{amati:2002:acm}}] considers the document as a simple bag of words. It is a model derived from the DFR framework, with the Equation (\ref{eq:pl2f}) formulated as
  $
  tfn_e=F_t\times log_2\left(1+c\times\frac{l_{avg}}{l_e}\right)
  $,
  where $c$ is a normalisation parameter.
  \item[\emph{PL2F}] is defined in Equation (\ref{eq:dfr-score}). It considers a document as a set of fields, each field being a bag of words.
\end{labeling}

The Table~\ref{tab:norm-param} reports the values of normalisation parameters for each ranking function. For a given function and dataset, the parameter value that maximises the Mean Average Precision (MAP) is found through a constrained particle swarm optimisation~\cite{xiaohui:2002:sci}. The optimisation is run on ranking functions that have all their weights equal to one, i.e., $\alpha_e = \alpha_a = \alpha_v = 1$. The reason is to limit what may impact the performance of a ranking function, thus invalidating the optimisation.\\

\input{05-ranking/figures/experiments/norm-param}

Using such parameters, we report in Figures~\ref{fig:bm25mf-field-cmp-bar}~and~\ref{fig:pl2mf-field-cmp-bar} the performance of the ranking functions on the three datasets, for BM25MF and PL2MF respectively. The raw results of the bar charts are available in the Appendix section, in Table~\ref{tab:ranking-cmp-results}.

Using the two-tailed Wilcoxon matched-pairs signed-ranks test~\cite{sheskin:2003:CRC,buttcher:2010:IRI:1869919}, the difference between a candidate ranking function and a MF extension is statistically significant at level $0.05$ if the bar is displayed with a \textit{dot} pattern; it is statistically significant at level $0.10$ if displayed with a \emph{oblique lines} pattern instead.\\

%We note that for field-based ranking models and their MF extensions, the attribute label is considered as a value node, in order to be a source of potential relevant terms.
TF-IDF provides a clear-cut discrepancy between INEX09 and the datasets based on BTC, i.e., SS10 and SS11, the reason being it is not suited for heterogeneous datasets.

On SS10, BM25MF (resp., PL2MF) does not report a significant difference with BM25 (resp., PL2). On INEX09 and SS11, the MF extensions provide an increase of at least $10\%$ in retrieval performance compared to BM25 and PL2.

On SS10 and SS11, the MF extensions provide better ranking performance with a significant difference than the field-based ranking functions with an increase of $15\%$ at the minimum. On INEX09, PL2F and its MF extension PL2MF provide similar ranking performance.

Overall, the experiments show that the MF model improves significantly the ranking effectiveness over traditional field-based methods. In addition, the experiments emphasize as well the fine tuning possible with the MF ranking model, allowing to better fit the ranking to a specific dataset.

\input{05-ranking/figures/experiments/mf-field-cmp-bar}

\subsection{Effectiveness of the Weights}
\label{sec:weights-effectiveness}

In this section, we evaluate the impact of the presented weights over the MF ranking extensions. First, we discuss the impact of discarding the attribute label as a source of possible relevant terms on the ranking performance. Then, we evaluate the weights from Section~\ref{sec:weights} developed for the MF model.

Results of the evaluations are depicted in Figure~\ref{fig:mf-weights}. The PL2MF results are displayed on the left side of the figures, and on the right side the results of BM25MF. Using the two-tailed Wilcoxon matched-pairs signed-ranks test~\cite{sheskin:2003:CRC,buttcher:2010:IRI:1869919}, the MF extension with no added weight is used as the base of the comparison. Bars with a dot pattern indicate a statistically significant difference at level $0.05$ compared to the MF extension; bars with a oblique lines pattern is at level $0.10$ instead. Raw results are reported in Table~\ref{tab:mf-effect} of the Appendix section.

\subsubsection{\emph{Impact of the Attribute Label}}
\label{sec:with-att}

In this section, we investigate the consequence of \textbf{not} considering the attribute label as a source of potentially relevant terms, i.e., the attribute is not expanded into a leaf as described in Section~\ref{chap:tree-ranking:mf-model}.

Figure~\ref{fig:mf-att} depicts the results, where we observe that removing the attribute label lowers the performance of the ranking with a statistical significance on INEX09 with BM25MF and PL2MF, and on SS11 with PL2MF only. This shows that the attribute labels can be a source of possible relevant terms.

\subsubsection{\emph{Query Coverage Weight}}
\label{sec:qc-weight-effect}

In order to evaluate the benefit of the query coverage weight QC, we first analyse its effect separately when applied as an entity, an attribute or a value-specific weight. Then we study the consequence of applying it on all nodes at the same time.

We observe in Figure~\ref{fig:mf-qc} that the QC weight improves the retrieval performance when applied on the attribute node, with a statistical significance on SS10 and SS11.

\subsubsection{\emph{Leaf Coverage Weight}}
\label{sec:lc-weight-effect}

The evaluation of the leaf coverage weight LC investigates its efficiency with and without the \emph{control} function (\ref{eq:lc-norm}), where we depict the results in Figure~\ref{fig:mf-lc}.

We provide for each dataset the best performing parameters described in Section~\ref{sec:leaf-coverage} for the control function. For the three datasets regardless of the MF extension, the values are as follows:
\begin{description}
	\item[INEX09:] $n=1\;\;\alpha=0.7$;
	\item[SS10:] $n=2\;\;\alpha=0.4$; and
	\item[SS11:] $n=1\;\;\alpha=0.9$.
\end{description}
We can observe that the LC weight, with the control function (\ref{eq:lc-norm}) applied, improves slightly the retrieval performance on SS10 and SS11. The reason is that without this function, LC assigns a low weight to leaves containing many terms, even if they have occurrences of all query terms.

\subsubsection{\emph{Attribute and Entity Labels Weights}}
\label{sec:ael-weight-effect}

For the weights that depend on the label of the entity or of an attribute, we use the following regular expressions to determine the value of the weight:
\begin{itemize}
	\item $2$ if the label matches "\verb/.*(label|name|title|sameas)$/";
	\item $0.5$ if the label matches "\verb/.*(seealso|wikilinks)$/";
	\item $0.1$ if the label matches "\verb|^http://www.w3.org/1999/02/22-rdf-syntax-ns#_\d+$|"; and
	\item $1$ otherwise.
\end{itemize}
For instance, if an attribute label is \url{http://xmlns.com/foaf/0.1/name} then a weight of $2$ is assigned. The weight's values are set empirically, and were chosen in order to improve the task of entity search. The rationale is that a document where a query term occurs associated with an attribute matching "title\$" should be promoted.

The third regular expression matches an attribute URI defining items of a collection in RDF\footnote{RDF Container: \url{http://www.w3.org/TR/rdf-schema/\#ch\_container}}. It is assigned a low weight of $0.1$ to reduce the importance of terms occurring in each item of the collection.\\

We evaluate the Attribute and Entity Labels (AEL) weights first by considering the Attribute and the Entity Label weights separately, then both at the same time. Figure~\ref{fig:mf-ael} depicts the results of applying such query-independent weights.

We note that the Attribute Label weight gives significant benefits to the ranking in SS10 and SS11, while it decreases the ranking performance in INEX09. This indicates that carefully defined weights for important and non-important attributes can contribute significantly to the effectiveness of the approach. We note also that the same can be seen with the Entity Label weight applied alone. The reason is similar to the Attribute Label weight.

Except for INEX09, using both weights at the same time increases the performance of MF ranking functions noticeably.

\subsubsection{\emph{Combination of Weights}}
\label{sec:combi-weight-effect}

In this section, we investigate the retrieval performance when all four weights are used together. We depict results in Figure~\ref{fig:mf-all}, with the weights configuration
\begin{enumerate}
    \item QC applied on the attribute node only;
    \item LC with dataset-specific $n$ and $\alpha$ parameters for the control function; and
    \item AEL weights.
\end{enumerate}

The weights applied on a same node are combined by the multiplication of each weight value on that node.
The attribute and entity label being leaves in the ranking model, we apply also the QC weight on those two in this experiment.

On INEX09 their combination decreases slightly the performance for PL2MF. On SS10 and SS11, although the QC and LC weights applied separately do not improve the effectiveness of the MF ranking functions by much, their combination with AEL increases the retrieval performance by at least $30\%$ on SS10 and SS11.

\begin{figure}
	\centering
	\begin{subfigure}{.483\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/experiments/cmp-w-wo-att}
		}
		\caption{Without the attribute as a leaf}
		\label{fig:mf-att}
	\end{subfigure}
	\quad
	\begin{subfigure}{.483\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/experiments/cmp-qc}
		}
		\caption{Query coverage weight QC applied on the leaf, on the attribute, on the entity, and on all three levels}
		\label{fig:mf-qc}
	\end{subfigure}
	\qquad
	\begin{subfigure}{.483\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/experiments/cmp-lc}
		}
		\caption{Leaf coverage weight LC, tested with and without the control function in Equation~(\ref{eq:lc-norm})}
		\label{fig:mf-lc}
	\end{subfigure}
	\quad
	\begin{subfigure}{.483\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/experiments/cmp-ael}
		}
		\caption{Entity and attribute labels weights}
		\label{fig:mf-ael}
	\end{subfigure}
	\qquad
	\begin{subfigure}{.8\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{05-ranking/figures/experiments/cmp-all}
		}
		\caption{All weights combined}
		\label{fig:mf-all}
	\end{subfigure}
	\caption[Evaluation of weights with the MF ranking extensions]{Evaluation of weights with the MF extensions. The PL2MF results are displayed on the left side of the figures, and the results of BM25MF are on the right side. Bars with a dot pattern indicate a statistically significant difference at level $0.05$ compared to the MF extension; bars with a oblique lines pattern is at level $0.10$ instead.}
	\label{fig:mf-weights}
\end{figure}

\section{Query-dependent Ranking of the Graph Summary}
\label{sec:summary-ranking}

Graph summarisation is used for extracting the underlying structure of graph-shaped data. However, the size of a summary can still be an obstacle towards an effective browsing of the data.

Yu and Yagadish propose in~\cite{yu:2006:schema-summarization} to ``summarise'' an \emph{existing} database schema by showing only the \emph{important} parts. This allows to get a succinct overview of the database.
However, that work proposes an approach that is independent of the actual information need of a user.

In the case of a graph summary, we consider the information need of a user to be expressed as a graph-shaped query. Given a query over the summary, we propose to rank its solutions using the MF ranking model. In this way, the user will first browse subgraphs of the summary that are most relevant to his need. In this section, we evaluate the MF ranking model applied to a summary, showing the ability of the two levels of Normalisations in MF for fine-tuning of the ranking function.

\subsection{Dataset}
\label{sec:summary-ranking:dataset}

In order to evaluate the ranking on a graph summary, we use the USEWOD2013 dataset \cite{usewod:2013}. This dataset provides query logs from several SPARQL endpoints. In this evaluation, we focus on the logs from the DBpedia SPARQL endpoint. SPARQL~\cite{PrudS08} is the standard query language for RDF data. Given a graph-shaped query, it retrieves all subgraphs from a dataset which match its pattern.

\subsubsection{Query Extraction}

The graph summary highlights the structure of the graph it was created from. Therefore, we are only interested in queries that pertain to the structure of the graph. We present in this section a function that takes as input a SPARQL query, applies a set of rules over it, and outputs as result the transformed SPARQL query. The transformed query is then devoid of any specific details about entities.

\begin{definition}[Query Extraction Function]
Let $I$ and $O$ be two SPARQL queries. Let $qe$ be a function that consists of the following rules:
\begin{enumerate}
	\item retain only the basic graph patterns~\cite{PrudS08} (BGP);
	\item transform CONSTRUCT and ASK queries~\cite{PrudS08} into \emph{wildcard} select queries;
	\item remove information specific to an entity; and
	\item discard any solution modifiers.
\end{enumerate}
The application of the function $qe$ on the query $I$ outputs the query $O$ that is independent of any entity.
\label{def:query-extraction-function}
\end{definition}

In the following, we describe each operation of the function $qe$ and consider the query below as an ongoing example of the effect of an operation on the query:

\begin{minted}[linenos,frame=lines,framesep=1mm]{sparql}
PREFIX dbr: <http://dbpedia.org/resource/>
PREFIX dbo: <http://dbpedia.org/ontology/>
PREFIX dbp: <http://dbpedia.org/property/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

ASK {
  dbr:Paris dbo:country "France" ;
            rdfs:label ?label .
  dbr:Eiffel_Tower dbp:owner dbr:Paris .
  FILTER isLiteral(?label)
}
ORDER BY ?label
\end{minted}

\paragraph{Operation 1.}

A basic graph pattern (BGP) represents a set of triple patterns, which together form a graph pattern. A triple pattern represents an edge of a graph, where any of its components can be a variable.
\begin{definition}[Triple Pattern]
Let $G=\left\langle V, A, l_V \right\rangle$ be a graph, $\mathcal{L}$ be the set of labels, and $Var$ an infinite set of variables.
A triple pattern is a tuple $t= \langle s, p, o \rangle$ where $t \in (\mathcal{L} \cup Var) \times (\mathcal{L} \cup Var) \times (\mathcal{L} \cup Var)$.
The components of a triple pattern $t$ are denoted $subject(t)$, $predicate(t)$ and $object(t)$, respectively.
An edge $(x,\alpha,y) \in A$ of the graph $G$ is a match for a triple pattern $t$ if and only if the non-variable components of $t$ are equal to those of the edge $(x, \alpha, y)$.
\label{def:triple-pattern}
\end{definition}

In this operation, we discard any component of the SPARQL query that is not a BGP, e.g., filters.
With regards to the ongoing example, the effect of this operation removes the FILTER pattern in line~10.
\begin{minted}[linenos,frame=lines,framesep=1mm]{sparql}
# Same set of prefixes
ASK {
  dbr:Paris dbo:country "France" ;
            rdfs:label ?label .
  dbr:Eiffel_Tower dbp:owner dbr:Paris .
}
ORDER BY ?label
\end{minted}

\paragraph{Operation 2.}

A SPARQL query can be either
\begin{inparaenum}[(i)]
	\item a \emph{CONSTRUCT}, i.e., a query that allows to create a graph from a solution;
	\item a \emph{ASK}, i.e., a query that returns a boolean value that is true if there was at least one match, and false otherwise; or
	\item a \emph{SELECT}, i.e., a query that returns a table, where a row represents a solution for the query and a column one of its variables.
\end{inparaenum}

In this experiment, we disregard the kind of output requested for a query. Also, the variables that may have been specifically selected in a SELECT query are not important here. Therefore, we view all queries as \emph{wildcard} SELECT queries, i.e., the binding of every variable in a solution is returned.

With regards to the ongoing example, this operation results in the query being transformed into a SELECT query.
\begin{minted}[linenos,frame=lines,framesep=1mm]{sparql}
# Same set of prefixes
SELECT * {
  dbr:Paris dbo:country "France" ;
            rdfs:label ?label .
  dbr:Eiffel_Tower dbp:owner dbr:Paris .
}
ORDER BY ?label
\end{minted}

\paragraph{Operation 3.}

An entity can be a person, a place, an abstract concept, etc. For this experiment, we are interested in queries that pertain to structure of the graph. Therefore, we replace with a \emph{unique} variable any detail a query might have about a specific entity.

If the detail is either a URI such as \href{http://dbpedia.org/resource/Paris}{<http://dbpedia.org/resource/Paris>}, a literal such as \emph{"Paris"}, then it is replaced with a unique variable. In the case where a literal is a type (Section~\ref{sec:ssd:type}), it is then kept as is.

With regards to the ongoing example, we replace in this operation the URIs \href{http://dbpedia.org/resource/Paris}{dbr:Paris} and \href{http://dbpedia.org/resource/Eiffel\_Tower}{dbr:Eiffel\_Tower} with the variables \emph{?uri\_1} and \emph{?uri\_2}, respectively. Also, the literal "France" is replace with the variable \emph{lit\_1}.
\begin{minted}[linenos,frame=lines,framesep=1mm]{sparql}
# Same set of prefixes
SELECT * {
  ?uri_1 dbo:country ?lit_1 ;
         rdfs:label ?label .
  ?uri_2 dbp:owner ?uri_1 .
}
ORDER BY ?label
\end{minted}

\paragraph{Operation 4.}

In SPARQL, one can use \emph{solution modifiers} in order to alter the result set of the query. For instance, the keyword \emph{LIMIT} is used for limiting the number of results up to a specified value. As for previous operations, a solution modifier does not impact on the graph structure; therefore, those are removed from the extracted queries.

Finally, this operation removes the solution modifier \emph{ORDER BY} from the ongoing example query.
\begin{minted}[linenos,frame=lines,framesep=1mm]{sparql}
# Same set of prefixes
SELECT * {
  ?uri_1 dbo:country ?lit_1 ;
         rdfs:label ?label .
  ?uri_2 dbp:owner ?uri_1 .
}
\end{minted}

\subsubsection{Query Classification}

We classify a query according to the complexity of a pattern. The complexity is based on two features of a query, which are
\begin{inparaenum}[(i)]
	\item the number of ``star-shaped'' patterns; and
	\item the size of a star-shaped pattern.
\end{inparaenum}

A star-shaped pattern --- or \emph{star} in short --- is a BGP where the \emph{subject} of all triple patterns is the same. The size of such a pattern is determined by the number of triple patterns it contains. For instance, the BGP ``?s rdfs:label ?label . ?s foaf:name ?name .'' is a star of size 2, since the subject of both triple patterns is ``?s''.

Such a classification scheme allows to capture two aspects of a graph summary that have an impact on its \emph{precision} introduced in Section~\ref{sec:error-classification}. The size of a star in a query pertains to the attribute and type error. In addition, the number of stars in a query pertains to the connectivity error.

\subsubsection{Observations of Queries in the Dataset}

We process the queries from the logs in the USEWOD2013 dataset using the function $qe$ from Definition~(\ref{def:query-extraction-function}).
Once the operations from the function $qe$ are performed, we have then a collection of queries that are only concerned about the structure of the graph queried.

Table~\ref{tab:usewod2013-queries} reports general statistics about the queries extracted from the USEWOD2013 DBpedia~3.3 logs using the function $qe$. The full list of queries is available in the Appendix~\ref{app:summary-ranking}. For each category of complexity as defined in the previous section, we report in the table
\begin{inparaenum}[(a)]
\item the number of queries;
\item the number of unique terms where a term is a URI or a literal; and
\item the distribution of queries across the categories as a percentage.
\end{inparaenum}

The queries are grouped by its complexity, where a group is identified with the a string having the following regular expression: "\verb/\d+(-\d+)*/". For instance, the string ``\emph{2-1}'' identifies a group of queries that have \textit{two} star-shaped patterns, one with \textit{two} triple patterns and the other with only \textit{one}.

Using the process outlined above, a total of 293 queries spanning over 17 categories were extracted. We note that the distribution of queries follows a power law. Indeed, most of the queries are contained in the categories ``2'', ``3'', and ``4''; those represent BGPs with only a single star-shaped pattern. Only 52 queries (18\%) actually contain more than one star. This suggests that a majority of the extracted queries are intended at retrieving \emph{entities}.

\begin{table}
	\centering
	\resizebox{.8\textwidth}{!}{
		\begin{tabular}{lc@{\hs}rc@{\hs}rc@{\hs}r}
			\toprule
			Category & \phantom{a} & Number of Queries & \phantom{a} & Number of Unique Terms & \phantom{a} & Query Distribution (in \%) \\
			\cmidrule{3-3} \cmidrule{5-5} \cmidrule{7-7}
			1-1-4 & \phantom{a} & 2 & \phantom{a} & 9 & \phantom{a} & 0.68 \\
			1-1-3 & \phantom{a} & 2 & \phantom{a} & 6 & \phantom{a} & 0.68 \\
			1-1-2 & \phantom{a} & 1 & \phantom{a} & 3 & \phantom{a} & 0.34 \\
			3-4 & \phantom{a} & 1 & \phantom{a} & 6 & \phantom{a} & 0.34 \\
			2-2 & \phantom{a} & 2 & \phantom{a} & 5 & \phantom{a} & 0.68 \\
			1-5 & \phantom{a} & 1 & \phantom{a} & 6 & \phantom{a} & 0.34 \\
			1-4 & \phantom{a} & 1 & \phantom{a} & 5 & \phantom{a} & 0.34 \\
			1-3 & \phantom{a} & 1 & \phantom{a} & 4 & \phantom{a} & 0.34 \\
			1-2 & \phantom{a} & 19 & \phantom{a} & 30 & \phantom{a} & 6.48 \\
			1-1 & \phantom{a} & 22 & \phantom{a} & 28 & \phantom{a} & 7.51 \\
			10 & \phantom{a} & 1 & \phantom{a} & 10 & \phantom{a} & 0.34 \\
			9 & \phantom{a} & 1 & \phantom{a} & 9 & \phantom{a} & 0.34 \\
			6 & \phantom{a} & 7 & \phantom{a} & 33 & \phantom{a} & 2.39 \\
			5 & \phantom{a} & 7 & \phantom{a} & 24 & \phantom{a} & 2.39 \\
			4 & \phantom{a} & 26 & \phantom{a} & 49 & \phantom{a} & 8.87 \\
			3 & \phantom{a} & 65 & \phantom{a} & 87 & \phantom{a} & 22.18 \\
			2 & \phantom{a} & 134 & \phantom{a} & 112 & \phantom{a} & 45.73 \\
			\midrule
			\textit{Total} & \phantom{a} & 293 & \phantom{a} & 201 & \phantom{a} & 100.00 \\
			\bottomrule
		\end{tabular}
	}
	\caption{Queries extracted from the USEWOD2013 query logs}
	\label{tab:usewod2013-queries}
\end{table}

\subsection{Graph Summary Ranking}
\label{sec:graph-summary-ranking}

While creating a summary of a given graph, one may accumulate several statistics. We generate a summary with the following statistics:
\begin{enumerate}
	\item the number of nodes mapped to a sumnode;
	\item the number of occurrences of an attribute associated to a specific sumnode;
	\item the number of times an attribute connects two sumnodes; and
	\item the number of types in a sumnode.
\end{enumerate}
Those statistics are used in the ranking function that we present below.

\begin{figure}
	\centering
	\resizebox{.6\textwidth}{!}{
		\input{05-ranking/figures/tikz/summary-ranking-ex.tex}
	}
	\caption{A solution to the SPARQL query of the running example. A number in parenthesis represents a statistic associated with that node.}
	\label{fig:sparql-solution}
\end{figure}

As an example, we consider the graph in Figure~\ref{fig:sparql-solution} that is a solution of the query from the previous section over a graph summary. A number between parenthesis indicates a statistic associated with the node. For instance, there is one node from the entity graph that got mapped to the sumnode $s_1$.

\paragraph{Ranking functions.}

We present the basic and MF-based approaches, two ranking functions that can be applied over a graph summary.

\subparagraph{Basic approach.}

A straightforward approach consists in summing up all the statistics available to a subgraph that matched a query. The rationale is that the greater the sum, the more relevant is that solution. The result is $5$ in Figure~\ref{fig:sparql-solution}.

%\subparagraph{Field-based approach.}

%In this approach, we apply the Normalisation~\ref{norm:content}. The consequence in Figure~\ref{fig:sparql-solution} is that the \emph{average cardinality} of the attribute ``dbp:owner'' is taken into consideration.

\subparagraph{MF-based approach.}

In this approach, we apply both normalisations available with the MF ranking model, i.e., Normalisations~\ref{norm:content} and~\ref{norm:degree}. The difference of this approach with previous is that the \emph{average cardinality} of every node is considered.

\subsection{Evaluation}
\label{sec:summary-ranking:eval}

In Section~\ref{sec:summary-ranking:dataset} we presented a dataset of queries that were extracted from logs of the DBpedia SPARQL endpoint. In Section~\ref{sec:summary-ranking:eval} we introduced two ranking functions that we apply on sub-graphs of a graph summary which is, in this evaluation, that of DBpedia.
In this experiment, we evaluate the performance of the MF-based approach at ranking subgraphs of a graph summary.

We present first the ranking paradigm that we follow, before describing the experiment itself. Then, we describe the evaluation method for asserting the performance of the MF extension. Finally, we discuss the results of the experiment.

\paragraph{Ranking paradigm.}

Given a SPARQL query, its solutions retrieved from a graph summary can be erroneous as expressed in Chapter~\ref{chap03:sec:quality}; indeed, a summary path from a solution might have no instance that form a path in the entity graph as well. A good solution is then a subgraph of the summary that does exist in the entity graph. Therefore, a ranking algorithm should rank first such solutions.

\paragraph{Experimental setting.}

Due to imprecisions in a summary as discussed in Chapter~\ref{chap03:sec:quality}, a query can return solutions that do not have any actual \emph{summary instance} in the entity graph. We the present the following experimental setting in order to evaluate the performance of the MF extension with regards to the ranking paradigm.\\

Given a SPARQL query extracted from the USEWOD2013 query logs of DBpedia, we replace with a variable that we denote as ``\emph{?POF}'' a term of the query, one at a time. A term is either a predicate or an object of a triple pattern. The resulting query is then executed over a graph summary of DBpedia.\\

In parallel, we run the query over the entity graph; this allows us to compare the bindings of the ?POF variables as returned from both graphs, i.e., the entity graph and its summary. The difference between the two sets of bindings is an indication of the summary's precision for that query.

We keep only the queries that have a solution over the DBpedia dataset; this ensures that any invalid binding of the ?POF variable is due to the summary.

\subparagraph{Example.}

Consider the SPARQL query taken as an example in the previous section. We replace the term \emph{rdfs:label} with the \emph{?POF} variable. Let us consider as well that the terms \emph{rdfs:label} and \emph{rdfs:comment} are bindings for that variable as retrieved from the entity graph; however, bindings retrieved from the summary are \emph{rdfs:label}, \emph{rdfs:comment} and \emph{dc:title}. The term \emph{dc:title} is then invalid.

%\begin{table}
%	\centering
%	\resizebox{.5\textwidth}{!}{
%		\begin{tabular}{lc@{\hs}rc@{\hs}rc@{\hs}r}
%			\toprule
%			Category & \phantom{a} & Min & \phantom{a} & Max & \phantom{a} & Average \\
%			\cmidrule{3-3} \cmidrule{5-5} \cmidrule{7-7}
%			10 & \phantom{a} & 213 & \phantom{a} & 448 & \phantom{a} & 241.50 \\
%			1-1-2 & \phantom{a} & 1 & \phantom{a} & 44514 & \phantom{a} & 11132.50 \\
%			1-1-3 & \phantom{a} & 1 & \phantom{a} & 44514 & \phantom{a} & 8923.80 \\
%			1-1-4 & \phantom{a} & 1 & \phantom{a} & 3369 & \phantom{a} & 987.58 \\
%			1-1 & \phantom{a} & 3 & \phantom{a} & 36684 & \phantom{a} & 6407.35 \\
%			1-2 & \phantom{a} & 3 & \phantom{a} & 44858 & \phantom{a} & 6568.68 \\
%			1-3 & \phantom{a} & 1 & \phantom{a} & 134 & \phantom{a} & 48.50 \\
%			1-4 & \phantom{a} & 77 & \phantom{a} & 845 & \phantom{a} & 534.00 \\
%			1-5 & \phantom{a} & 566 & \phantom{a} & 1706 & \phantom{a} & 1294.33 \\
%			2-2 & \phantom{a} & 132 & \phantom{a} & 2926 & \phantom{a} & 1386.62 \\
%			2 & \phantom{a} & 3 & \phantom{a} & 144103 & \phantom{a} & 23107.88 \\
%			3-4 & \phantom{a} & 147 & \phantom{a} & 4455 & \phantom{a} & 1179.14 \\
%			3 & \phantom{a} & 26 & \phantom{a} & 143859 & \phantom{a} & 9733.65 \\
%			4 & \phantom{a} & 26 & \phantom{a} & 49682 & \phantom{a} & 5239.40 \\
%			5 & \phantom{a} & 57 & \phantom{a} & 12793 & \phantom{a} & 1875.20 \\
%			6 & \phantom{a} & 57 & \phantom{a} & 5250 & \phantom{a} & 454.26 \\
%			9 & \phantom{a} & 213 & \phantom{a} & 448 & \phantom{a} & 244.67 \\
%			\bottomrule
%		\end{tabular}
%	}
%	\caption{Queries extracted from the USEWOD2013 query logs}
%	\label{tab:usewod2013-queries:coverage}
%\end{table}

\paragraph{Ranking performance measure.}

We consider two aspects of the ranking in this evaluation:
\begin{enumerate}
	\item whether a solution that provides bindings for the \emph{?POF} variable that are part of the error set (Section~\ref{sec:error-classification}) is ranked lower that one that does not; and
	\item the more a solution returns elements from the error set, the lower ranked it is.
\end{enumerate}
In order to achieve this, we use the Mean Average Precision~\cite{manning:2008:iir} (MAP) as the measure of ranking performance.
The average solution metric considers the \emph{order} of a solution in the rank list, in addition to whether a solution is relevant or not.
The MAP measure averages that metric over all the queries that were executed.

\paragraph{Discussion.}

We compare in this paragraph the performance of both ranking functions when applied over the Types summary of DBpedia. First, we evaluate the impact of the normalisation parameters on the MF-based ranking approach. Then, we discuss the ranking performance of the basic and MF-based approaches.

\subparagraph{Normalisation parameters impact.}

Figures~\ref{fig:summary-ranking-norm} depict the impact of the Normalisation~\ref{norm:content} and~\ref{norm:degree} parameters on the ranking of graphs. On the X axis we vary the value of the parameter $b_a$ for Normalisation~\ref{norm:degree}. Each curve corresponds to a value of the parameter $b_v$ fro Normalisation~\ref{norm:content}. On the Y axis we report the MAP measurement for the MF-based ranking approach. The raw results of the curves are available in Appendix~\ref{tab:summary-ranking-norm}.\\

In this experiment, we divided the queries into two categories: simple graphs and complex graphs. Simple graphs indicate queries that have only one star, i.e., the queries from the complexity categories 10, 9, 6, 5, 4, 3, and 2. Complex graphs represents queries that contain more than one star, i.e., the queries with complexity categories 1-1-4, 1-1-3, 1-1-2, 3-4, 2-2, 1-5, 1-4, 1-3, 1-2, and 1-1.

Figure~\ref{fig:summary-ranking-norm-simple} depicts the \emph{average} of the MAP measurements from the seven queries classes of the simple category. Figure~\ref{fig:summary-ranking-norm-complex} depicts the \emph{average} of the MAP measurements from the ten queries classes within the complex category.\\

We observe that with simple graphs in Figure~\ref{fig:summary-ranking-norm-simple} the parameter $b_a$ for Normalisation~\ref{norm:degree} reported on the X axis has little effect on the ranking. On the contrary, we see it has a positive impact in Figure~\ref{fig:summary-ranking-norm-complex} for complex graphs.
The parameter $b_v$ for Normalisation~\ref{norm:content} increases the performance of the ranking as its value reaches $1$.
This shows the normalisation parameters introduced with the MF ranking model allow a fine tuning of the ranking to the data at hand.

\input{05-ranking/figures/summary-group-norm}

\subparagraph{Basic and MF-based approaches comparison.}

Given the outcome of the previous experiment, we set to $1.0$ the parameters of the Normalisations in the MF-based approach for the comparison between MF-based and basic ranking approaches of the graph summary.\\

Table~\ref{tab:summary-ranking-cmp} reports the MAP values of the basic and MF-based approaches over the Types summary of Dbpedia. We write the MAP value in bold for the approach that has the greatest. We observe that for a majority of the query complexities, the MF-based approach provides a better ranking than the basic approach.

This shows the benefits of the MF ranking model of normalising the data over two levels: the Normalisation~\ref{norm:content} allows to compare several solutions to a query regardless of different statistics they might have; with the Normalisation~\ref{norm:degree}, it is possible to rank solutions to a query that have different degree of a same attribute.

\input{05-ranking/figures/summary-ranking-comp}

%\begin{figure}
	%\centering
	%\resizebox{.7\textwidth}{!}{
		%\begin{tikzpicture}
		%\begin{axis}[
			%compat=newest,
			%ymajorgrids,
			%ybar,
			%symbolic x coords={$1-1-4$,$1-1-3$,$1-1-2$,$3-4$,$2-2$,$1-5$,$1-4$,$1-3$,$1-2$,$1-1$,$10$,$9$,$6$,$5$,$4$,$3$,$2$},
			%enlargelimits=0.15,
			%ylabel={MAP},
			%x=2cm,
			%bar width=0.3cm,
			%xtick=data,
			%legend style={at={(0.5,-0.15)},
				%anchor=north,legend columns=-1},
		%]
		%\addplot coordinates {
			%($1-1-4$, 0.8096)
			%($1-1-3$, 0.8692)
			%($1-1-2$, 1.0000)
			%($3-4$,   1.0000)
			%($2-2$,   0.7503)
			%($1-5$,   0.9784)
			%($1-4$,   0.8793)
			%($1-3$,   0.6730)
			%($1-2$,   0.8795)
			%($1-1$,   0.9381)
			%($10$,    0.9351)
			%($9$,     0.9344)
			%($6$,     0.7877)
			%($5$,     0.8475)
			%($4$,     0.8685)
			%($3$,     0.8763)
			%($2$,     0.9320)
		%};

		%\addplot coordinates {
			%($1-1-4$, 0.8102)
			%($1-1-3$, 0.8698)
			%($1-1-2$, 1.0000)
			%($3-4$,   1.0000)
			%($2-2$,   0.7870)
			%($1-5$,   0.9791)
			%($1-4$,   0.8867)
			%($1-3$,   0.6744)
			%($1-2$,   0.8977)
			%($1-1$,   0.9447)
			%($10$,    0.9439)
			%($9$,     0.9433)
			%($6$,     0.8699)
			%($5$,     0.9111)
			%($4$,     0.9089)
			%($3$,     0.9154)
			%($2$,     0.9389)
		%};

		%\legend{Basic,MF-based}
		%\end{axis}
		%\end{tikzpicture}
	%}
	%\caption{Comparison of the basic and MF-based ranking approaches on the Types summary of DBpedia 3.3}
%\end{figure}

