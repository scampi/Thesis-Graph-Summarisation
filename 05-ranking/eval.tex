\section{Experiments}
\label{sec:experiments}

In order to evaluate the MF model, we perform several experiments using three different datasets. We start by experimenting on the normalization parameters in order to study their impact on the effectiveness of the approach. We then compare the MF ranking functions against other traditional ones. We finally discuss the consequence of \textbf{not} considering the attribute label as a source of potential relevant terms and demonstrate the effectiveness of the proposed weights.

While there are other ways to perform entity ranking on the Web of Data, e.g., one can look at the other SemSearch 2011\footnote{SemSearch2011: \url{http://km.aifb.kit.edu/ws/semsearch11}} candidates, in this evaluation we concentrate on demonstrating how the MF model specifically extends and improves the very popular PRF and DFR frameworks.

\subsection{Datasets}
\label{sec:datasets}

The datasets we are using in our experiments are the following:
\begin{description}
  \item[INEX09] a dataset of 2,491,134 triples from DBpedia containing the description of entities in English, and converted for the INEX evaluation framework~\cite{Perez-Aguera:2010:UBS};
  \item[SS10] the ``Billion Triple Challenge''\footnote{Billion Triple Challenge: \url{http://vmlion25.deri.ie}} (BTC) dataset, containing more than one billion triples, with the assessments of the SemSearch2010\footnote{SemSearch2010: \url{http://km.aifb.kit.edu/ws/semsearch10}} challenge;
  \item[SS11] the BTC dataset with the assessments of the ``Entity Search track'' of the SemSearch2011\footnote{SemSearch2011: \url{http://km.aifb.kit.edu/ws/semsearch11/}} challenge.
\end{description}
The INEX09 dataset is significantly different than the other two based on BTC. Indeed, BTC is a heterogeneous dataset, created from web crawls of several search engines. INEX09 is a highly curated dataset from DBpedia.

%\subsection{Effectiveness of the MF Ranking Model}
%\label{sec:mv-fields-effectiveness}
%
% Next we perform a comparison between the MF extensions, i.e., BM25MF and PL2MF, against other ranking approaches. 

\subsection{\emph{The Normalization Parameters}}
\label{sec:norm-exp}

In this section, we study the impact on the ranking of the normalization parameters. In addition to the length normalization of field-based ranking function (Normalisation~\ref{norm:content}), the MF ranking model offers an additional normalization on the attribute's cardinality (Normalisation~\ref{norm:degree}). The effectiveness of the PRF and DFR frameworks depends on finding the right values for the normalization parameters. However, these parameters are highly dependent on the dataset.\\

Figures~\ref{fig:bm25mf-norm} and~\ref{fig:pl2mf-norm} depict the impact of the normalization parameters on the ranking performance of BM25MF and PL2MF, respectively. For all three datasets, a figure depicts on the \textbf{Y} axis the Mean Average Precision (MAP) scores, and on the \textbf{X} axis the Normalization~\ref{norm:content}, i.e., with parameter $b_v$ in Figure~\ref{fig:bm25mf-norm} (resp., parameter $c_v$ in Figure~\ref{fig:pl2mf-norm}). Each curve plots the results with a fixed Normalization~\ref{norm:degree} parameter, i.e., $b_a$ in Figure~\ref{fig:bm25mf-norm} and $c_a$ in Figure~\ref{fig:pl2mf-norm}.

The grid of parameters values in Figure~\ref{fig:bm25mf-norm} ranges from $0$ to $1$ with a step of $0.1$. In Figure~\ref{fig:pl2mf-norm}, the grid ranges from $0.5$ to $10.5$ with a step of $1$. Although these parameters can be attribute and value-specific, this experiment considers a constant parameter in order to reduce the number of combinations and to lower the variability of the results. Dashed lines depict the MAP scores of BM25F and PL2F and solid lines the scores of their MF extension, BM25MF and PL2MF respectively.\\

These plots show that Normalisation~\ref{norm:degree}, the normalisation introduced with the MF ranking model, improves the performance. Indeed, the Normalisation~\ref{norm:content} parameters $b_v$ and $c_v$ traditional to the field-based ranking model do not grasp completely the heterogeneity in the data, which results in lower performance when compared to the MF extensions. This indicates that the distinction between leaves within an attribute has a positive effect on the ranking.

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\input{05-ranking/figures/tikz/norm-exp-bm25mf}
		\addtocounter{subfigure}{-1}
		\caption{Evaluation of BM25MF normalization parameters. A curve plots a fixed $b_v$ value (Equation~(\ref{bm25mf_v})) with $b_a$ (Equation~(\ref{bm25mf_a})) varying from $0$ to $1$ with a precision step of $0.1$.}
		\label{fig:bm25mf-norm}
	\end{subfigure}
	\qquad
	\begin{subfigure}{\textwidth}
		\centering
		\input{05-ranking/figures/tikz/norm-exp-pl2mf}
		\addtocounter{subfigure}{-1}
		\caption{Evaluation of PL2MF normalization parameters. A curve plots a fixed $c_v$ value (Equation~(\ref{eq:pl2mf_v})) with $c_a$ (Equation~(\ref{eq:pl2mf_a})) varying from $0.5$ to $10.5$ with a precision step of $1$.}
		\label{fig:pl2mf-norm}
	\end{subfigure}
	\caption{Impact of the normalisation parameters on the MF ranking functions. The figures report the MAP values of the respective datasets on the Y axis. On the X axis and with each curve, we vary the values of normalisation parameters.}
\end{figure}

\subsection{Comparison between MF and Field-Based Models}
\label{sec:mf-field-cmp}

In this section, we evaluate and compare the performance of BM25 and PL2 ranking model against their MF extensions, BM25MF and PL2F respectively, and show the superiority of the MF model. The TF-IDF function is used as a baseline.

\begin{labeling}{\emph{BM25~\cite{robertson:1994:sigir}}}
  \item[\emph{TF-IDF}] is a logarithmic function of the term frequency and defines the Equation (\ref{eq:tfidf-score}) as 
  $
  tfn=log(F_t)+1
  $,
  where $F_t$ is the number of occurrences of the term $t$ in the entity.
  \item[\emph{BM25~\cite{robertson:1994:sigir}}] considers the document as a simple bag of words. It is a function of the term frequency derived from a two-Poisson model and it uses an entity-length normalization. The entity length is computed as the sum of the \emph{field length} defined in the Section~\ref{sec:ranking-wod}.
  
  It defines the Equation (\ref{eq:tfidf-score}) as 
  $
  tfn=\frac{F_t\times(k_1+1)}{F_t+k_1\times \left(1+b\times\left(\frac{l_e}{l_{avg}}-1\right)\right)}
  $,
where $l_e$ is the \emph{entity length} of the entity $e$, $l_{avg}$ is the average of the \emph{entity length} in the collection and $b$ is a normalization parameter.
  \item[\emph{BM25F}] is defined in Equation (\ref{eq:tfidf-score}). It considers documents as composed of fields, each field being a bag of words.
  \item[\emph{PL2~\cite{amati:2002:acm}}] considers the document as a simple bag of words. It is a model derived from the DFR framework, with the Equation (\ref{eq:pl2f}) formulated as 
  $
  tfn=F_t\times log_2\left(1+c\times\frac{l_{avg}}{l_e}\right)
  $,
  where $c$ is a normalization parameter.
  \item[\emph{PL2F}] is defined in Equation (\ref{eq:dfr-score}). It considers a document as a set of fields, each field being a bag of words.
\end{labeling}

The Table~\ref{tab:norm-param} reports the values of normalization parameters for each ranking function. For a given function and dataset, the parameter value that maximises the Mean Average Precision (MAP) is found through a constrained particle swarm optimisation~\cite{xiaohui:2002:sci}. The optimisation is run on ranking functions that have all their weights equal to one, i.e., $\alpha_e = \alpha_a = \alpha_v = 1$.\\

\input{05-ranking/figures/experiments/norm-param}

Using such parameters, we report in Figures~\ref{fig:bm25mf-field-cmp-bar}~and~\ref{fig:pl2mf-field-cmp-bar} the performance of the ranking functions on the three datasets, for BM25MF and PL2MF respectively. The raw results of the bar charts are available in the Appendix section, in Table~\ref{tab:ranking-cmp-results}.

Using the two-tailed Wilcoxon matched-pairs signed-ranks test~\cite{sheskin:2003:CRC,buttcher:2010:IRI:1869919}, the difference between a candidate ranking function and a MF extension is statistically significant at level $0.05$ if the bar is displayed with a \textit{dot} pattern; it is statistically significant at level $0.10$ if displayed with a \emph{grid} pattern instead.\\

%We note that for field-based ranking models and their MF extensions, the attribute label is considered as a value node, in order to be a source of potential relevant terms.
TF-IDF provides a clear-cut discrepancy between INEX09 and the datasets based on BTC, i.e., SS10 and SS11, the reason being it is not suited for heterogeneous datasets.

On SS10, BM25MF (resp., PL2MF) does not report a significant difference with BM25 (resp., PL2). On INEX09 and SS11, the MF extensions provide an increase of at least $10\%$ in retrieval performance compared to BM25 and PL2.

On SS10 and SS11, the MF extensions provide better ranking performance with a significant difference than the field-based ranking functions with an increase of $15\%$ at the minimum. On INEX09, PL2F and its MF extension PL2MF provide similar ranking performance.

Overall, the experiments show that the MF model improves significantly the ranking effectiveness over traditional field-based methods.

\input{05-ranking/figures/experiments/mf-field-cmp-bar}

\subsection{Effectiveness of the Weights}
\label{sec:weights-effectiveness}

In this section, we discuss the impact of discarding the attribute label as a source of possible relevant terms on the ranking performance. Then we evaluate the weights from Section~\ref{sec:weights} developed for the MF model.
The Table~\ref{tab:mf-effect} reports the MAP scores of BM25MF and PL2MF combined with each weight individually and using the normalization parameters values from the Table~\ref{tab:norm-param}. Apart from the row ``Without Attribute Label'', all runs consider the attribute label as an additional value as in the previous experiments.

\subsubsection{\emph{The Impact of Attribute Label}}
\label{sec:with-att}

In this section, we investigate the consequence of not considering the attribute label as a source of relevant terms.
The Table~\ref{tab:mf-effect} reports under the \emph{BM25MF} and \emph{PL2MF} methods the results of considering or not the attribute label as an additional value.
We observe that removing the attribute label (\emph{Without Attribute Label} row) lowers the performance of the ranking with a statistical significance on INEX09 with BM25MF and PL2MF, and on SS11 with PL2MF only. This shows that the attribute labels can be a source of possible relevant terms.

\subsubsection{\emph{The Query Coverage Weight}}
\label{sec:qc-weight-effect}

In order to evaluate the benefit of the QC weight, we first analyse its effect separately when applied as an entity, an attribute or a value-specific weight. Then we study the consequence of applying it on all nodes at the same time (\emph{All} row). The Table~\ref{tab:mf-effect} reports the results under the \emph{BM25MF + QC} and \emph{PL2MF + QC} methods. QC improves the retrieval performance when applied on the attribute node, with a statistical significance on SS10 and SS11.

\subsubsection{\emph{The Leaf Coverage Weight}}
\label{sec:lc-weight-effect}

The evaluation of the LC weight investigates its efficiency with and without the function (\ref{eq:lc-norm}). The results are reported in Table~\ref{tab:mf-effect} under the \emph{BM25MF + LC} and \emph{PL2MF + LC} methods. We provide for each dataset the best performing $B$ and $\alpha$ parameters. We can observe that LC with the function (\ref{eq:lc-norm}) improves slightly the retrieval performance on SS10 and SS11. The reason is that, without this function, LC assigns a low weight to long values even if they have occurrences of all query terms.

\subsubsection{\emph{The Attribute and Entity Labels Weights}}
\label{sec:ael-weight-effect}

We evaluate the AEL weights first by considering the Attribute and the Entity Label weights separately, then both at the same time. The Table~\ref{tab:mf-effect} reports the results of applying such query-independent weights under the \emph{BM25MF + AEL} and \emph{PL2MF + AEL} methods.
We note that the Attribute Label weight gives significant benefits to the ranking in SS10 and SS11, while it decreases the ranking performance in INEX09. This indicates that carefully defined weights for important and non-important attributes can contribute significantly to the effectiveness of the approach. We note also that the same can be seen with the Entity Label weight applied alone. The reason is similar to the Attribute Label weight.
Except in INEX09, using both weights at the same time increases the performance of MF ranking functions noticeably.

\subsubsection{\emph{The Combination of Weights}}
\label{sec:combi-weight-effect}

In this section, we investigate the retrieval performance when all four weights are used together. We report the results in Table~\ref{tab:mf-effect} under the methods  \emph{BM25MF + QC + LC + AEL} and \emph{PL2MF + QC + LC + AEL}, with the weights configuration
\begin{inparaenum}[(1)]
    \item QC applied on the attribute node;
    \item LC with dataset-specific $B$ and $\alpha$ parameters; and
    \item AEL weights.
\end{inparaenum}
The weights applied on a same node are combined by the multiplication of each weight value on that node, i.e., either $b_a$ (resp., $c_a$) or $b_v$ (resp., $c_v$) weight values.
The attribute label being considered as a value, and the entity label being an additional attribute, we apply also the QC weight on those two labels in this experiment.
On INEX09 their combination decreases slightly the performance for PL2MF. On SS10 and SS11, although the QC and LC weights applied separately do not improve the effectiveness of the MF ranking functions by much, their combination with AEL increases the retrieval performance by at least $30\%$ on SS10 and SS11.

\input{05-ranking/figures/experiments/weights-cmp}
