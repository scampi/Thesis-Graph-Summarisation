\section{Graph Summary Generation}
\label{chap4:summary:algo}

In this section, we present an algorithm for generating a graph summary. Some optimisations are possible depending on the relation. However, the proposed approach is independent of the \gls{summarisation-relation} used. The algorithm takes a graph as input and outputs a graph summary, which is distinct from the input graph.
We first present a version of the algorithm for a single dataset and then for a collection of several datasets in Section~\ref{sec:summary-algos}.
Next, we describe in Section~\ref{sec:summary-impl} two implementations of the algorithm, one based on MapReduce~\cite{dean:2004:msd}, and the other on SPARQL.

\subsection{Algorithms}
\label{sec:summary-algos}

The creation of a graph summary is decomposed into four steps. These steps are common to both versions of the algorithm, i.e., the graph summarisation of a single dataset and of a collection of datasets. In a first step, we aggregate all the information about a node needed for the \gls{summarisation-relation}. In a second step, we create a table that associates a node of $G$ to a sumnode. In a third step, we materialise the sumedges by joining the previous table with the entity graph. The fourth step is optional and consists in gathering statistics about the summarised graph.

\begin{labeling}{\textbf{\underline{Step 2:}} \emph{Node to sumnode mapping.}}
	\item[\textbf{\underline{Step 1:}} \emph{Entity description.}]
	\phantomsection
	\label{step-ed}
	In order to create a graph summary, the basic information we manipulate is an entity. The computation of the entity description \glssymbol{edesc} requires a pass over the edges, with a worst case complexity of $O(\vert A \vert)$. The entity description provides the necessary contextual information needed for the \gls{summarisation-relation}.

	\item[\textbf{\underline{Step 2:}} \emph{Node to sumnode mapping.}]
	\phantomsection
	\label{step-hn}
	A sumnode unique identifier is defined based on the entity description \glssymbol{edesc}, with regards to the \gls{summarisation-relation}.
	%This is the core operation of the \texttt{GetSumnode} function, which assigns a sumnode to a node.
	For example, a unique identifier can be created from the \gls{types} of an entity for the relation $R_t$.
	The worst case complexity of this operation is $O(\vert V \vert)$, since we need to visit each node of the graph.

	\item[\textbf{\underline{Step 3:}} \emph{Sumedge materialisation.}]
	\phantomsection
	\label{step-he}
	We need to join the previous mapping of a node to its sumnode with the entity graph in order to materialise an sumedge. We do this by two joins, i.e., on the source and target nodes. In this step, we need to visit each edge in the graph, so in the worst case it has a $O\left(\vert A \vert\right)$ complexity.

	\item[\textbf{\underline{Step 4:}} \emph{Statistics gathering.}]
	\phantomsection
	\label{step-stats}
	During the previous steps, it is possible to gather statistics about the graph. This step is optional, since depending on the application in which the summary is used statistics might be unnecessary. However, gathering statistics comes at no cost, since we can re-use computations of the previous steps. Indeed in Step~2, we can keep the count of nodes that are mapped to a sumnode. In addition in Step~3, we can count the occurrences of an edge between two sumnodes. Such statistics provide insight into the graph structure, complementing the graph summary.
\end{labeling}

\subsubsection{Graph Summarisation of a Single Dataset}
\label{sec:alg-single-ds}

We present here the graph summarisation algorithm that is aimed at processing a single dataset.
%The Figure~\ref{fig:gs-algo} depicts the overall algorithm for generating a graph summary.
The Algorithm~\ref{chap4:summary:alg:summary} outlines the flow of the graph summarisation which takes a graph as input and outputs its summary for a given \gls{summarisation-relation}.

The \gls{summarisation-relation} is first materialised into a table where each row report two elements: a node and its corresponding sumnode as per the \gls{summarisation-relation}.
For each edge of the graph, the algorithm first retrieves the sumnodes corresponding to the source and target nodes thanks to the \texttt{GetSumnode} function. This function performs a lookup on the previous materialised table. We note that since the summary of a graph is unique for a given \gls{summarisation-relation}, the sumnode can be retrieved deterministically.

Then, the corresponding sumedge is added to the summary. The lines~2-5 represent the first two steps of the graph summary creation, and the line~7 corresponds to the materialisation of the sumedge.

\begin{remark}
On line~5 we retrieve the sumnode of $v$ only if it is not a type, since we consider the type as a feature of the \gls{summarisation-relation}.
\end{remark}

%\begin{figure}
%	\centering
%	\input{04-summary/figures/algo}
%	\caption{Graph summarisation flow. $(u, \alpha, v) \in A$ is an edge of $G$, $(d, u, x) \in D \times V \times \mathcal{W}$ is a tuple associating a node $u$ to a sumnode $x$ within a dataset $d$. We use the summarisation relation only within the ``Sumnode Creation'' step.}
%	\label{fig:gs-algo}
%\end{figure}

\begin{algorithm}
	\DontPrintSemicolon
	\SetKwFunction{sn}{GetSumnode}
	\KwIn{A graph $G=\left\langle V, A, l_V \right\rangle$}
	\KwOut{A summary $\mathcal{S}=\left\langle \mathcal{W}, \mathcal{B}, l_\mathcal{W} \right\rangle$ of $G$}
	\BlankLine
	\ForEach{$(u, \alpha, v) \in A$}{
		\tcp{Sumnode of the source node $u$}
		$S_u \gets \sn(u)$ \;
		\tcp{Sumnode of the target node $v$}
		$S_v \gets \varnothing$ \;
		\If(\tcp*[h]{$\alpha$ is not an attribute type}){$\alpha \not \in \mathcal{L}^T$}{
			$S_v \gets \sn(v)$ \;
		}
		\tcp{Build the graph summary}
		$\mathcal{W} \gets \mathcal{W} \cup \{ S_u, S_v \}$ \;
		$\mathcal{B} \gets \mathcal{B} \cup \{ (S_u, \alpha, S_v) \}$ \;
	}
	\caption{Graph summarisation of a single dataset}
	\label{chap4:summary:alg:summary}
\end{algorithm}

\subparagraph{Example.}

In the Figure~\ref{tab:algo-ex}, we depict the \emph{Types} summarisation $R_t$ of a graph showing people and documents. We group the edges describing a same node in \hyperref[step-ed]{Step 1}. In \hyperref[step-hn]{Step 2} we assign the sumnode \emph{h1} to the nodes (i.e., \emph{:e1} and \emph{:e3}) of type \emph{:Person}, and \emph{h2} to the node (i.e., \emph{:e2}) of type \emph{:Document}. In \hyperref[step-he]{Step 3}, we join the edges of the input graph with the table created in \hyperref[step-hn]{Step 2} in order to retrieve the sumnodes.
It is possible to gather statistics about the summarisation in \hyperref[step-stats]{Step 4} by, e.g., grouping over the sumnodes and counting the number of rows in a group. Figure~\ref{chap4:summary:fig:algo-ex} depicts the \emph{Types} summary that is the output of the algorithm which execution is depicted in Figure~\ref{tab:algo-ex}.

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{04-summary/figures/algo-ex}
		}
		\caption{The input data is a set of edges, one per row. The columns $u_\mathcal{W}$ and $v_\mathcal{W}$ represents the sumnodes associated with the node $u$ and $v$, respectively. The Step~2 assigns the sumnode \emph{h1} to the nodes \emph{e1} and \emph{e3}; and the sumnode \emph{h2} to the node \emph{e3}. These sumnodes are joined with the input edges in Step~3 in order to materialise the sumedges.}
		\label{tab:algo-ex}
	\end{subfigure}
	\qquad
	\begin{subfigure}{.7\textwidth}
		\centering
		\resizebox{.7\textwidth}{!}{
			\input{04-summary/figures/fig-algo-ex}
		}
		\caption{Depiction of the \emph{Types} summary created in Figure~\ref{tab:algo-ex}.}
		\label{chap4:summary:fig:algo-ex}
	\end{subfigure}
	\caption{Example of the \emph{Types} summarisation $R_t$ in the case of a single dataset.}
\end{figure}

\subsubsection{Graph Summarisation of Inter-Linked Datasets}
\label{chap4:summary:impl:inter-datasets}

The Web Data is a collection of semi-structured data that hail from a variety of sources. Sources may provide overlapping information and reference each other. In such an environment, questions of trust about the legality of information arise, e.g., whether some description associated to a node in the graph is accurate or not. Since the graph summary is built from (inter-linked) data, it may present a structure of the graph that is not expected for a given dataset. We discuss in the next paragraph challenges presented in summarising inter-lined datasets.

\subparagraph{Challenge in summarising heterogeneous inter-linked datasets.}

The information about an entity, e.g., a person, a place, an organisation, \ldots, can be spread across several datasets. In the Web of Data this happens when a same entity URI is reused across datasets. The \hyperref[step-hn]{Step 2} of the graph summarisation relies on the entity description \glssymbol{edesc} in order to create the corresponding sumnode. Some edges of the description might be erroneous, which would then impact negatively on the summary. A solution is to keep track of the \emph{origin} of an edge and act accordingly during the summarisation process.

In Figure~\ref{chap4:summary:fig:sum-issue} we depict the summarisation of a graph which edges are spread over the datasets \emph{D1} and \emph{D2}. The former contains the edges $\{ (v_1, a, Person), (v_1, name, John) \}$, while the latter contains $\{ (v_1, a, Book), (v_1, title, Rama) \}$. Since the same node $v_1$ is used in \emph{D1} and \emph{D2}, all four edges contribute to the \gls{summarisation-relation}. That node is then mapped to the sumnode $T_1$ according to the \emph{Types} summarisation relation $R_t$.
Due to the overlap over \emph{D1} and \emph{D2} of $v_1$'s entity description, the sumnode reports that it is of both types \emph{Person} and \emph{Book}, as well as both \gls{attributes} \emph{name} and \emph{title}.

Therefore, information about an entity within a dataset can be erroneous since it is unlikely that a same node is typed both a \emph{Person} and a \emph{Book}. It is likely that either the graph in \emph{D1} or in \emph{D2} is incorrect; this leads to a misleading graph summary.
In order to prevent this issue, we need to differentiate edges within \emph{D1} from those in \emph{D2}.

\begin{figure}
	\centering
	\resizebox{.8\textwidth}{!}{
		\input{04-summary/figures/summary-ild}
	}
	\caption[Challenge in summarising heterogeneous data sources with the \emph{Types} summarisation relation $R_t$]{Challenge in summarising heterogeneous data sources with the \emph{Types} summarisation relation $R_t$. The datasets D1 and D2 contain edges that describe a same node labelled $v_1$.}
	\label{chap4:summary:fig:sum-issue}
\end{figure}

\subparagraph{Edge authority.}

In this section, we introduce the concept of \emph{edge authority} which is needed for the summarisation of inter-linked datasets. The proposed definition may not fit every application, as it was crafted to fit our view of inter-linked datasets.

\begin{labeling}{\emph{Edge Provenance:}}
	\item[\emph{Edge Provenance:}] Due to the principle of Linked Data which states that anything can be said about anything, there is a need to know the provenance of an edge. This necessity goes in accordance with the N-Quads\footnote{\url{http://www.w3.org/TR/n-quads/}} serialisation format. The provenance of an edge indicates the dataset it is originally from.

	\item[\emph{Node Ownership:}] To assess the information given by an edge, we need to know the ownership of a node in addition to the edge's provenance. This concept differs from the \emph{provenance} of an edge in the sense that a dataset may contain edges about a node, but not necessarily owns that node.
\end{labeling}

\begin{definition}[Edge Provenance]
	Let $G = \left\langle V, A, l_V \right\rangle$ be a graph. The provenance of an edge in $G$ is the dataset label of $G$, i.e., $\glssymbol{Glabel}(G)$.
\end{definition}

\begin{definition}[Node Ownership]
	Let $G = \left\langle V, A, l_V \right\rangle$ be a graph. The ownership of a node is the function $\glssymbol{dsource} : V \mapsto \mathcal{L}$ which maps a node to a dataset label.
\end{definition}

\begin{remark}
	In the case of inter-linked datasets, the provenance of edges that describe an entity may differ from the ownership of that entity.
\end{remark}

The authority of an edge combines the concepts of provenance and ownership, in order to determine whether that edge is ``legal'' or not. An edge is legal if the provenance of the edge is the same as the ownership of the source node.

\begin{definition}[Edge Authority]
	Let $G = \left\langle V, A, l_V \right\rangle$ be a graph.
	An edge $(u, \alpha, v) \in A$ of the graph $G$ has authority if $\glssymbol{dsource}(u) = \glssymbol{Glabel}(G)$.
	\label{chap4:summary:def:edge-authority}
\end{definition}

\begin{remark}
	In RDF, an edge which source node is a blank node has authority implicitly, since it is local to the dataset it originates from.
\end{remark}

The Figure~\ref{chap4:summary:fig:authority} depicts three cases of authority. A dataset is represented by a colour and a line type, i.e., there are three datasets {\bfseries A}, {\bfseries B}, and {\bfseries C} which are dotted red, solid blue, and dashed green, respectively. The provenance of an edge determines its colour and line type. The ownership of a node is determined visually by the dataset it is in. We refer to the graphs in datasets \textbf{A}, \textbf{B}, \textbf{C} as $G_A$, $G_B$, and $G_C$, respectively.\\

The ownership of the nodes $a_1$ and $a_2$ is dataset {\bfseries A}, and it is dataset {\bfseries B} for the node $b_1$. Both edges $(a_1, \alpha, a_2)$ and $\left(a_1, \beta, b_1 \right)$ have \emph{authority} since
\begin{inparaenum}[(1)]
	\item the ownership of the source node $a_1$ is the dataset {\bfseries A}, i.e., $\glssymbol{dsource}(a_1) = \textbf{A}$; and
	\item the provenance of both edges is also dataset {\bfseries A}, i.e., $\glssymbol{Glabel}(G_A) = \textbf{A}$.
\end{inparaenum}
Since $\glssymbol{dsource}(a_1) = \glssymbol{Glabel}(G_A)$, both edges aforementioned have authority.\\

The edge $\left(a_2, \gamma, b_1\right)$ does not have authority, since its provenance is {\bfseries B} and the ownership of its source node $a_2$ is {\bfseries A}. Similarly, the edge $\left(a_1,\delta,b_1\right)$ does not have authority either, since its provenance (dataset \textbf{C}) is not equal to the ownership of node $a_1$ (dataset \textbf{A}).

\begin{figure}
	\centering
	\input{04-summary/figures/authority}
	\caption[Edge authority]{Edge authority. A dataset is represented by a colour and a line type. The provenance of an edge determines its colour and line type. The ownership of a node is determined visually by the dataset it is in.}
	\label{chap4:summary:fig:authority}
\end{figure}

\subparagraph{Algorithm.}

%Step2
%In order to compute the summary over a collection of datasets, we need to record the dataset label from where the entity description originates from. The output of this step is a list of tuples, i.e., $\left\lbrace (d, u, x) \in D \times V \times \mathcal{W} \right\rbrace$ where $d$ is a dataset label, $u$ a node and $x$ a sumnode. We remark that the sumnode is ``local'' to the dataset, since information about an entity can be scattered over several datasets.
%Step3
%Although this would duplicate some elements of the list of tuples, the various data sources for an entity forces it. This occurs whenever datasets link to each other.

In order to summarize a collection of inter-linked datasets, we extend the Algorithm~\ref{chap4:summary:alg:summary} with the dataset label information.
%We augment the edges of the input graphs with their provenance.
%The Algorithm~\ref{chap4:summary:alg:id-summary} modifies the function \texttt{GetSumnode} so to make it aware of the dataset label component.
The Algorithm~\ref{chap4:summary:alg:id-summary} modifies the function \texttt{GetSumnode} so to implement the edge authority.
We name it as \texttt{GetSumnode\textsubscript{i}} to mark this change, where the subscript \emph{i} denotes the dataset label.
We extract the features for the \gls{summarisation-relation} from the edges in the entity description \glssymbol{edesc} only if one of the following conditions is met:
\begin{enumerate}
\item the edge has authority; or
\item the provenance of the edge is $G_i$.
\end{enumerate}
By doing so, we retain the summarisation features from each dataset. This ensures we can summarise a dataset that uses a node which ownership is external to it, without corrupting the information about that node.% The features which have authority are always used in order to support the case where a dataset reuses a node it didn't provide the necessary summarisation features for (e.g., types).

The Algorithm~\ref{chap4:summary:alg:id-summary} takes a collections graphs from $n$ datasets, and outputs a summary that describes the structure of graphs within and between datasets. This algorithm is the same as the Algorithm~\ref{chap4:summary:alg:summary}, but with the redefined \texttt{GetSumnode\textsubscript{i}} function. Given a node $u \in V$, that function considers the edges from its entity description and filters out those that do not have authority (lines~3-4).

\begin{algorithm}
	\DontPrintSemicolon
	\SetKwProg{Fn}{Function}{:}{end}
	\SetKwFunction{sn}{GetSumnode\textsubscript{i}}
	\KwIn{A collection of $n$ graphs: $\forall 0 < i < n\; G_i=\left\langle V_i, A_i, l_{V_i} \right\rangle$. Let $G =\left\langle V, A, l_{V} \right\rangle$ be the union of all graphs, i.e., $A = A_1 \cup \cdots \cup A_n$ and $V = V_1 \cup \cdots \cup V_n$}
	\KwOut{A sumnode of the summary $\mathcal{S}=\left\langle \mathcal{W}, \mathcal{B}, l_\mathcal{W} \right\rangle$ of $G$}
	\BlankLine
%	\ForEach{$(u, \alpha, v)_i \in A$}{
%		\tcp{Sumnode of the source node $u$}
%		$S_u \gets \sn{u}$ \;
%		\tcp{Sumnode of the target node $v$}
%		$S_v \gets \varnothing$ \;
%		\If(\tcp*[h]{$\alpha$ is not an attribute type}){$\alpha \not \in \mathcal{L}^T$}{
%			$S_v \gets \sn{v}$ \;
%		}
%		\tcp{Build the graph summary}
%		$\mathcal{W} \gets \mathcal{W} \cup \{ S_u, S_v \}$ \;
%		$\mathcal{B} \gets \mathcal{B} \cup \{ (S_u, \alpha, S_v) \}$ \;
%	}
%	\BlankLine
	\tcp{The \sn function considers the features that have authority and those which provenance is $G_i$}
	\Fn{\sn{$u \in V$}}{
		\ForEach(\tcp*[h]{For every edge in the entity description of $u$}){$e \in \glssymbol{edesc}(u)$}{
			\uIf(\tcp*[h]{The edge $e$ has authority}){$\glssymbol{Glabel}(e) = \glssymbol{dsource}(u)$}{
				\tcp{Extract features for the summarisation relation from $e$}
			}
			\uElseIf(\tcp*[h]{The provenance of edge $e$ is $G_i$}){$\glssymbol{Glabel}(e) = \glssymbol{Glabel}(G_i)$}{
				\tcp{Extract features for the summarisation relation from $e$}
			}
		}
	}
	\caption{Graph summarisation of a inter-linked datasets}
	\label{chap4:summary:alg:id-summary}
\end{algorithm}

\subparagraph{Example.}

In Figure~\ref{tab:id-algo-ex} we depict the summarisation $R_t$ of two inter-linked datasets, \textbf{A} and \textbf{B}, where a person in the former is related to a person in the latter. The resulting \emph{Types} summary is depicted on the Figure~\ref{chap4:summary:fig:id-algo-ex}, where the edges follows the same representation of the datasets, i.e., those which provenance is \textbf{A} are solid red, and those which provenance is \textbf{B} are dashed blue.

The type features \emph{Student} and \emph{Person} of the node \texttt{e2} are shared between the datasets \textbf{A} and \textbf{B}, respectively. Since that node ownership is dataset \textbf{B}, the feature which provenance is \textbf{B} has authority, i.e., \emph{Person}, while the other in \textbf{A}, i.e., \emph{Student}, has not. In order to retain the information provided by dataset \textbf{A} about that node without corrupting the information gathered by the summary, we map the node \texttt{e2} according to the provenance information:
\begin{itemize}
\item for edges which provenance is \textbf{B}, calling \texttt{GetSumnode\textsubscript{B}($e_2$)} returns the sumnode \texttt{h1}; and
\item for edges which provenance is \textbf{A}, calling \texttt{GetSumnode\textsubscript{A}($e_2$)} returns the sumnode \texttt{h2}.
\end{itemize}

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{04-summary/figures/id-algo-ex}
		}
		\caption{The input data is a set of edges, one per row. The column ``Prov'' indicates the provenance of the edge. The columns $u_\mathcal{W}$ and $v_\mathcal{W}$ represents the sumnodes associated with the node $u$ and $v$, respectively. The ownership of \texttt{e1} is A, and the dataset B owns \texttt{e2}. The Step~2 assigns the sumnodes according to the function \texttt{GetSumnode\textsubscript{i}} in Algorithm~\ref{chap4:summary:alg:id-summary}.}
		\label{tab:id-algo-ex}
	\end{subfigure}
	\quad
	\begin{subfigure}{.8\textwidth}
		\centering
		\resizebox{.8\textwidth}{!}{
			\input{04-summary/figures/fig-id-algo-ex}
		}
		\caption{Depiction of the \emph{Types} summary created in Figure~\ref{tab:id-algo-ex}. The edges which provenance is A are solid red, and those which provenance is B are dashed blue.}
		\label{chap4:summary:fig:id-algo-ex}
	\end{subfigure}
	\caption{Example of the \emph{Types} summarisation $R_t$ in the case of inter-linked datasets.}
\end{figure}

\subsection{Implementations}
\label{sec:summary-impl}

We present in this section two implementations of the graph summarisation, a first one based on SPARQL and then a second based on MapReduce~\cite{dean:2004:msd}. We present only the implementations of the Algorithm~\ref{chap4:summary:alg:summary} for the summarisation of a single dataset, since they can be easily extended to the Algorithm~\ref{chap4:summary:alg:id-summary}. We outline below the required operators of the graph summarisation algorithm.
\begin{labeling}{\textbf{\underline{Step 1:}}}
\item[\textbf{\underline{Step 1:}}] In this step, we need a \emph{group} operator in order to build the entity description \glssymbol{edesc}.
\item[\textbf{\underline{Step 2:}}] In this step we create the sumnode associated to a node according to its entity description. Therefore, we need a \emph{projection} operator in order to extract the features of the \gls{summarisation-relation}. We note that a feature can be multi-valued, which needs to be taken into account as well. The creation of the sumnode from the extracted features requires an \emph{object invention}~\cite{hull:1989:usi} operator.
\item[\textbf{\underline{Step 3:}}] The materialisation of the sumedges requires a \emph{join} operator through which we can retrieve the sumnode(s) associated with the source and target nodes.
\item[\textbf{\underline{Step 4:}}] As in Step~1 we need a group operator in order to aggregate sumnodes and sumedges, possibly computing \emph{statistics} at the same time.
\end{labeling}

\subsubsection{SPARQL implementation}

In this section, we describe an implementation of the graph summarisation algorithm based on SPARQL. The SPARQL query language is rich enough for us to only rely on it for the summary generation.
The Figure~\ref{chap4:summary:fig:sparql-gs} shows the SPARQL query for generating a graph summary, and the query in Figure~\ref{chap4:summary:fig:relation-sparql} is an example of a \gls{summarisation-relation}, i.e., \emph{Types} summarisation relation $R_t$.

\minisec{Operators}

SPARQL provides the operator \texttt{GROUP BY} which can be used to group data over a set of variables. The projection operator is implemented using a \texttt{SELECT} query. In the case of a multi-valued variable, we use the \texttt{GROUP\_CONCAT} operator which allows to concatenate all the values into a single literal. The object invention is performed thanks to built-in hash functions. In a SPARQL query, patterns which reuse some variables for either subject, predicate, or object components create implicitly a join. The computation of statistics can be performed using the built-in aggregate function in conjunction with the \texttt{GROUP BY} operator, e.g., \texttt{COUNT}. Thanks to the CONSTRUCT operator, we are able to create the graph summary directly from the query.

\minisec{Step 1 and Step 2}

In the Figure~\ref{chap4:summary:fig:relation-sparql}, we extract the features relevant for the relation, e.g., the type values in this example. We note the use of the \texttt{ORDER BY} in the Figure~\ref{chap4:summary:fig:relation-sparql} so to ensure that an unique ``identifier'' is created for the set of features thanks to the built-in hash function \texttt{SHA1}. We use the aggregate \texttt{GROUP\_CONCAT} in order to pass a single literal to SHA1, since an entity can be associated with multiple types.

\minisec{Step 3}

In the Figure~\ref{chap4:summary:fig:sparql-gs}, the lines 4-7 (resp., lines 14-17) represent the \gls{summarisation-relation}, of which the Figure~\ref{chap4:summary:fig:relation-sparql} is an example. The first block associates a sumnode to the variable on the subject position \emph{?s}, and the second block to the variable on the object position \emph{?o}, changing the name of projected variables appropriately for the object in the query of Figure~\ref{chap4:summary:fig:relation-sparql}.

The sumedge materialisation is done on the line~11: the sumnode of the source node is retrieved thanks to the variable \emph{?s} that is projected by the sub-query in line~6; similarly for the target node thanks to the variable \emph{?o} returned by the sub-query line~16.
On lines 9 and 19, we create a URI for the source and target sumnodes, respectively.

On line 22, we consider the case when there is no sumnode associated with the object variable. This can happen if the object is a literal, or if no feature can be extracted for the resource. Since we are concerned with the graph structure with the graph summary, we need to keep the type value. Therefore, we add the condition on line 25 which corresponds to the line~4 in Algorithm~\ref{chap4:summary:alg:summary}.
The empty string in the \emph{else} clause represents the sumnode $\varnothing$ in the graph summary.

\begin{remark}
This works only if the ontology is not inside the processed dataset, since the type value would be lost by the sumnode URI creation.
\end{remark}

\minisec{Step 4}

It is possible to assign some statistics about the summarisation when generating the sumnodes and sumedges. This requires again additional \texttt{GROUP BY} operations that are not depicted in the figure. However, we need then to reify the statistics in RDF since we use the \texttt{CONSTRUCT} operator in the query. This is further discussed in the Section~\ref{chap7:applications:wdm:summary-rdf}.

\begin{remark}
If we use \texttt{CONSTRUCT} as the query form and we want to keep statistics about the summarisation, we need first to wrap the query into another one in which the aggregation is done. Then, we apply the \texttt{CONSTRUCT} query form on top. The reason is that that query form does not allow for aggregation operators such as \texttt{COUNT} in its clause.
\end{remark}

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		{\footnotesize
			\begin{minted}[linenos,frame=lines,framesep=4mm]{sparql}
SELECT ?s (SHA1(GROUP_CONCAT(?t; separator = ",")) AS ?sID) {
    SELECT DISTINCT ?s ?t {
        ?s a ?t
    }
    ORDER BY ?t
}
GROUP BY ?s
			\end{minted}
		}
		\caption{Summarisation relation $R_t$}
		\label{chap4:summary:fig:relation-sparql}
	\end{subfigure}
	\qquad
	\begin{subfigure}{\textwidth}
		\centering
		{\footnotesize
			\input{04-summary/figures/sparql.tex}
		}
		\caption{SPARQL query that generates the graph summary in the \texttt{CONSTRUCT} clause}
		\label{chap4:summary:fig:sparql-gs}
	\end{subfigure}
	\caption{SPARQL-based graph summarisation over a single dataset}
	\label{chap4:summary:fig:gs-sparql-all}
\end{figure}

\subsubsection{MapReduce Implementation}
\label{sec:mapreduce}

In this section, we describe an implementation of the graph summarisation algorithm based on MapReduce~\cite{dean:2004:msd}, which is a batch-processing framework that eases distributed operations over massive amount of data. The unit of work in MapReduce, called a \emph{job}, is composed of two operations: a \emph{mapper} and a \emph{reducer}. The mapper emits key-value pairs, and the reducer receives all the pairs associated to a same key.

\paragraph{Cascading.}

We developed the solution using Cascading\footnote{\url{http://www.cascading.org/projects/cascading/}}, which is a feature-rich API for defining and executing complex, scale-free, and fault-tolerant data processing workflows on a MapReduce cluster (e.g., Hadoop\footnote{http://hadoop.apache.org/}). The API lets the developer to quickly assemble complex processes without having to worry about the MapReduce paradigm. The Cascading model is based on the processing of ``tuples'', which can be seen as database records, thanks to operators that filter, join, aggregate, \ldots.  In the algorithms to follow, we represent a tuple using the set notation $\{\ldots\}$.

A Cascading flow is defined as a pipeline of such operators that connects data ``sources'' to ``sinks'' outputs. A flow can be composed of several ``pipes'', which represent a set of operations. The Cascading flow forms a directed acyclic graph, which is then converted into a sequence of MapReduce jobs that can be executed on the cluster.

The Figure~\ref{chap4:summary:fig:summary-cascading} depicts the Cascading flow of the graph summarisation. A node represent an operation over the incoming tuples, which if superscripted with \emph{GB} involves an aggregate operation. An aggregate function combines multiple tuples into a single one; the function may be to count the tuples that were aggregated in order to add statistics about the graph to the summary for example. The triangle-shaped nodes represent points in the flow where the data is written to or read from.

\minisec{Dictionaries}

Read and write (I/O) operations on disk are a major cause of performance degradation of MapReduce jobs. In order to avoid moving the data across the MapReduce cluster, we compute a set of \emph{dictionaries} that map a \emph{unique number} to a vocabulary term, i.e., either an attribute or a type. We then use this unique number throughout the graph summary computation, decreasing the amount of data copied across the cluster, but also improving operations such as joins.

The Figure~\ref{chap4:summary:fig:dict-cascading} depicts the Cascading flow for creating the dictionaries. The edges $(u, \alpha, v) \in A$ of the graph $G$ are parsed, then a fork is done with regards to the attribute. We use the HFile~\cite{hfile} data structure as the dictionary backend. In our experiments, this marked a significant performance improvement compared to the Hadoop's MapFile~\cite{mapfile}. We use the DistributedCache\footnote{\url{https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/filecache/DistributedCache.html}} in order to make the dictionaries accessible to all nodes of the MapReduce cluster.

The unique number a dictionary uses is computed using the MurmurHash3~\cite{murmurhash3-gcode,murmurhash3-blog} hash function. To further improve the performance of the summarisation, we hash the identifier of the nodes, i.e., the URI in the case of RDF data. A majority of the computation involves comparisons against these identifiers; thus using numbers instead of plain text improves shuffling operations in the MapReduce framework. In order to reduce potential hash collisions, we use the \emph{128bit} version of MurmurHash3.

Thanks to the dictionaries, the computation load over the two expensive join operations is decreased. Indeed, we only need to join data composed of unique numbers, without copying the plain text data across the MapReduce cluster which would increase the I/O. The entity description is needed only for the sumnode creation step. Once created, the computation can be abstracted from the actual data and use unique numbers instead. We use the dictionaries in the last step of the summarisation, which consist of the reification of the graph summary in RDF, intended for human consumption.

\minisec{Step 1}

The computation of a sumnode identifier requires all the information about an entity. Since the input data consist of an edge per tuple, we need to compute the entity description \glssymbol{edesc} which is depicted by the Step~1 in the Figure~\ref{fig:va-cascading}. After parsing the input data, we aggregate the tuples based on the entity $u$. Cascading provides the \emph{GroupBy} operator to do so.
%In the case of summarising a collection of datasets, we include the dataset label as the group key, in order to handle correctly the statement authority.

Depending on the \gls{summarisation-relation}, more or less information about an entity is needed. In the relations presented in this thesis such as the \emph{IO Attributes} relation $R_{ioa}$, attributes of \emph{incoming} edges are required. We retrieve such information with no extra MapReduce job compared to others. The difference is in the amount of data aggregated.

The Algorithm~\ref{chap4:summary:alg:inv-edge} describes the mapper function of the entity description creation to which we add the incoming edges. The mapper function takes the edge as input, and emits a tuple containing three elements. The underlined element represents the \emph{key} of the tuple. In the reducer function, we collect all the edges having the same hash key. If necessary, we can differentiate between incoming and outgoing edge thanks to the emitted flag.

\begin{algorithm}
	\DontPrintSemicolon
	\SetKwProg{Fn}{Function}{:}{end}
	\SetKwFunction{hash}{hash}
	\SetKwFunction{emit}{emit}
	\SetKwFunction{map}{map}
	\KwIn{a tuple containing an edge $(u, \alpha, v) \in A$.}
	\KwOut{a tuple containing\begin{inparaenum}[(1)]
			\item the hash value of the entity;
			\item a flag indicating whether the edge is incoming; and
			\item the edge.
		\end{inparaenum}}
	\BlankLine
	\tcp{Underlined tuple elements define the key within the MapReduce framework}
	\Fn{\map{$\left\lbrace\;(u, \alpha, v)\;\right\rbrace$}}{
		\emit{\{ \underline{\hash{$u$}}, 0, $(u, \alpha, v)$ \}}\;
		\tcp{``Reverse'' the edge direction}
		\emit{\{ \underline{\hash{$v$}}, 1, $(v, \alpha, u)$ \}}\;
	}
	\caption{Entity description expanded with incoming edges}
	\label{chap4:summary:alg:inv-edge}
\end{algorithm}

\minisec{Step 2}

In order to generate unique identifiers for sumnodes, we extract features relevant to the \gls{summarisation-relation} from the entity description. For instance, we need the type values for the relation $R_t$, and for $R_a$ we are interested in the attributes. The projection operator as well as the handling of multi-valued feature are then performed programmatically, using a \emph{mapper}.

The Algorithm~\ref{chap4:summary:alg:mapper-rt} describes the implementation of the $R_t$ relation, which stands as an example of Step~2 in the figure. We retrieve the type values associated with the node $u$ on lines~2-5. The object invention is performed on line~6, where we emit the input tuple to which we add the identifier of the sumnode, i.e., the hash of the set of types.

\begin{algorithm}
	\DontPrintSemicolon
	\SetKwProg{Fn}{Function}{:}{end}
	\SetKwFunction{hash}{hash}
	\SetKwFunction{emit}{emit}
	\SetKwFunction{map}{map}
	\SetKwData{types}{types}
	\KwIn{a tuple containing the hash value of a node $u$, and the entity description $\glssymbol{edesc}(u)$ of that node.}
	\KwOut{a tuple containing\begin{inparaenum}[(1)]
			\item the sumnode identifier;
			\item the hash value of $u$; and
			\item the entity description $\glssymbol{edesc}(u)$ of that node.
		\end{inparaenum}}
	\BlankLine
	\Fn{\map{$\{ \hash{u}, \glssymbol{edesc}(u) \}$}}{
		\types $\gets$ [ ]\;
		\ForEach{$(u, \alpha_i, v_i) \in \glssymbol{edesc}(u)$}{
			\If(\tcp*[h]{$\alpha_i$ is a type attribute}){$\alpha_i == \glssymbol{atype}$}{
				$\types[i] \gets v_i$\;
			}
		}
		\emit{$\{ \hash{\types}, \hash{u}, \glssymbol{edesc}(u) \}$}\;
	}
	\caption{\emph{Types} summarisation relation $R_t$}
	\label{chap4:summary:alg:mapper-rt}
\end{algorithm}

\minisec{Step 3}
\label{chap4:summary:algo:edge-materialisation}

We materialise the sumedges in two distinct processes, depicted by the Step~3a and Step~3b in the figure. The reason is to decrease the amount of data joined with the sumnode mappings from Step~2. To do so, we filter out in Step~3b the edges which target node is a sink --- in RDF, this means to remove the edges which object is a \emph{literal}. Also, we filter edges which have the type attribute. The reason is that the target node, which here is the type value, does not represent an entity. This is represented by the line~4 of the Algorithm~\ref{chap4:summary:alg:summary}.

In Step~3a, we create the sumedges which \emph{target} sumnode is $\varnothing$. In Step~3b instead, we consider the sumedges which target might be defined --- it may not be if the target node is not mapped to any sumnode (Step~2 in the figure). We remark that the Step~3a requires no join. Indeed, we keep the entity description along with the sumnode identifier as a result of the Step~2 processing.

\minisec{Step 4}

We end both Step~3a and Step~3b with a grouping operation. In Step~3a, we group all the tuples sharing the same sumnode identifier into a single tuple, computing some statistics if necessary, e.g., number of occurrences of an attribute, or the number of \emph{nodes} mapped to the sumnode. In Step~3b, we group the tuples sharing both the source and target sumnodes as well as the attribute, thus forming the sumedges. Similarly, we may compute statistics about the sumedge in this step, e.g., the number of occurrences of a labelled edges connecting any nodes that were mapped to the adjacent sumnodes.

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\resizebox{.6\textwidth}{!}{
			\input{04-summary/figures/dict}
		}
		\caption{Dictionary computation}
		\label{chap4:summary:fig:dict-cascading}
	\end{subfigure}
	\qquad
	\begin{subfigure}{\textwidth}
		\centering
		\resizebox{\textwidth}{!}{
			\input{04-summary/figures/flow}
		}
		\caption{Sumedge and sumnode creation}
		\label{fig:va-cascading}
	\end{subfigure}
	\caption[Graph summarisation using the Cascading framework]{Graph summarisation using the Cascading framework. A node represent an operation over the incoming tuples, which if superscripted with GB involves an aggregate operation, i.e., there is a \texttt{GROUP BY} operation. The $*$ superscript over the RDF nodes indicates that the dictionary is used for mapping the data back into plain text. The triangle-shaped nodes represent points in the flow where the data is written to or read from.}
	\label{chap4:summary:fig:summary-cascading}
\end{figure}

\subsubsection{Discussion}

Apart from the use of machine expensive operators such as \texttt{ORDER BY} or \texttt{GROUP\_CONCAT}, we remark that the SPARQL query of the \gls{summarisation-relation} is duplicated two times. Therefore, this is an inefficient part of the query depicted in Figure~\ref{chap4:summary:fig:sparql-gs}. Depending on the query planner of the SPARQL engine, this duplication might be spotted so to avoid computing twice the same pattern.

In comparison, the MapReduce-based one allows the summarisation to scale to much larger graphs. Indeed, in our experiments the SPARQL-based implementation is able to summarise graphs up to 20M edges only. A major obstacle to scale to larger graphs are the constraints imposed by SPARQL endpoints, e.g., limiting the number of rows returned by a \texttt{SELECT} query form, or query execution time-outs. Instead, the MapReduce implementation is able to scale to billions of edges, e.g., 2B with Freebase\footnote{\url{https://www.freebase.com/}} or even 30B with the Sindice dataset.

%\begin{figure}
%	\pgfplotstableread{
%		x         y    y-max  y-min
%		10        0.209219 0.34681420 0.17105213
%		50 3.823154 4.95525438 3.56146301
%		100 1.790687 3.17752483 1.64086531
%		500 8.759389 16.14254738 8.10480235
%	}{\mytable}
%	\begin{tikzpicture}
%	\begin{axis} [
%	ymin=0,
%	symbolic x coords={10,50,100,500},
%	xtick=data
%	]
%	\addplot [ybar, fill=gray!50]
%	plot [error bars/.cd, y dir=both, y explicit]
%	table [y error plus=y-max, y error minus=y-min] {\mytable};
%	\end{axis}
%	\end{tikzpicture}
%\end{figure}
